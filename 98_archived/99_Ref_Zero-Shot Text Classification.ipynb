{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mathematical-biography",
   "metadata": {},
   "source": [
    "# Adventures in Zero-Shot Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-metropolitan",
   "metadata": {},
   "source": [
    "Transfer learning has had an enormous impact in Natural Language Processing. Thanks to models like BERT, it is now possible to train more accurate NLP models than before, and typically do so with less labeled data. Now that finetuning language models has become the standard procedure in NLP, it’s only natural to get curious and ask: do we need any task-specific labeled training items at all?\n",
    "\n",
    "In this notebook, we investigate two available models for zero-shot text classification and evaluate how they perform. The code for this article is available in [our repository of NLP notebooks](https://nlptown.github.io/nlp-notebooks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-costs",
   "metadata": {},
   "source": [
    "## Zero-shot text classification\n",
    "\n",
    "Zero-shot and few-shot NLP models take transfer learning to the extreme: their goal is to make predictions for an NLP task without having seen one single labeled item (for zero-shot learning), or very few such items (for few-shot learning) specific to that task. The most well-known example is doubtlessly [OpenAI’s GPT-3](https://arxiv.org/abs/2005.14165), which has proved to be a very successful few-shot learner for a wide range of applications. While running GPT-3 lies beyond the means of most developers, luckily several smaller alternatives are available.\n",
    "\n",
    "In 2020, [Flair](https://github.com/flairNLP/flair) and [Transformers](https://huggingface.co/transformers/), two of the most popular NLP libraries, both added zero-shot classification to their offering. Flair, on the one hand, makes use of a so-called TARS classifier, short for [Text-Aware Representation of Sentences](https://kishaloyhalder.github.io/pdfs/tars_coling2020.pdf), which can be run with just a few lines of code:\n",
    "\n",
    "```\n",
    "classifier = TARSClassifier.load('tars-base')\n",
    "sentence = Sentence('Your example text')\n",
    "classifier.predict_zero_shot(sentence, [label1, label2, …])\n",
    "```\n",
    "\n",
    "Transformers, on the other hand, makes it possible to use a range of models from the [Hugging Face model hub](https://huggingface.co/models) in their `zero-shot-classification` pipeline:\n",
    "\n",
    "```\n",
    "classifier = pipeline('zero-shot-classification', model=\"your-nli-model\", device=0)\n",
    "classifier('Your example text', [label1, label2, …])\n",
    "```\n",
    "\n",
    "Despite the obvious similarities, the two implemented classifiers approach zero-shot text classification quite differently.\n",
    "\n",
    "The zero-shot pipeline in the Transformers library treats text classification as natural language inference (NLI). This approach was pioneered by [Yin et al. in 2019](https://arxiv.org/abs/1909.00161). In NLI, a model takes two sentences as input &mdash; a premise and a hypothesis &mdash; and decides whether the hypothesis follows from the premise (`entailment`), contradicts it (`contradiction`), or neither (`neutral`). For example, the premise _David killed Goliath_ entails the hypothesis _Goliath is dead_, is contradicted by _Goliath is alive_ and doesn’t allow us to draw any conclusions about _Goliath is a giant_. This NLI template can be reused for text classification by taking the text we’d like to label as the premise, and rephrasing every candidate class as a hypothesis. For a task such as polarity classification, the premise could be an opinion like _I loved this movie_, with the hypotheses _This sentence is positive_, _This sentence is negative_ or _This sentence is neutral_. The classifier will then determine the relationship between the premise and every hypothesis. In single-label classification, all resulting `entailment` scores are softmaxed to identify the single most probable class; in multi-label classification, the scores for `entailment` and `contradiction` are softmaxed for every label independently, so that several relevant labels can be identified.\n",
    "\n",
    "The TARS classifier in the Flair library takes a different course. Similar to the previous approach, it abstracts away from the specificities of individual classification tasks by feeding both the label and the text as input to a BERT classifier, separated by the `[SEP]` token. The main difference lies in the fact that this BERT model is not finetuned for NLI, but for a generic version of text classification. This is done by training the model to label every input pair as either true or false. To make sure it can handle a variety of classification tasks, Flair’s TARS classifier is finetuned on nine different datasets, with applications ranging from polarity to topic classification. For single-label classification, only the class with the highest score for `True` is selected as the final prediction; for multi-label classification, all classes with the prediction `True` are returned.\n",
    "\n",
    "Although both approaches to zero-shot classification sound very attractive, they share one disadvantage: in contrast to traditional text classification, each input text requires several forward passes through the model &mdash; one for each candidate label. The models are therefore less computationally efficient than more traditional text classifiers. Still, if they can bypass the need for expensive data labeling, for many applications this may be a small price to pay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-climb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The more pressing question is therefore how well zero-shot text classification works exactly. To find out, we evaluated the classifiers above on five different tasks, from topic classification to sentiment analysis. We used four datasets that are all available from the [Hugging Face datasets hub](https://huggingface.co/datasets),\n",
    "making sure that none of these datasets was used to finetune Flair’s TARS classifier. From each we selected 1,000 random test items:\n",
    "\n",
    "- [yahoo_answers_topics](https://huggingface.co/datasets/yahoo_answers_topics): questions and answers from Yahoo Answers, classified into 10 topics, such as `Society & Culture` and `Science & Mathematics`. As the input to the model we use the best answer only (without the question).\n",
    "- [banking 77](https://huggingface.co/datasets/banking77): a set of online user queries from the banking domain, each labeled with one of 77 intents. This is a challenging dataset, as the intents (such as `card_about_to_expire` and `card_not_working`) are very fine-grained.\n",
    "- [tweet_eval](https://huggingface.co/datasets/tweet_eval): English tweets labeled for a variety of tasks. We tested if the models can predict the emotion &mdash; `anger`, `joy`, `optimism` or `sadness` &mdash; and the sentiment polarity of the tweets &mdash; `positive`, `negative` or `neutral`.\n",
    "- [financial_phrasebank](https://huggingface.co/datasets/financial_phrasebank): financial news sentences (such as _Sales have risen in other export markets_) with a polarity label &mdash; `positive`, `negative` or `neutral`. We only selected sentences for which all annotators agreed on the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-chocolate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20195d37e6544ba09eca0aac63ee2d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7061166875194e1c82ff721cc53ffe09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/847 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to C:\\Users\\patri\\.cache\\huggingface\\datasets\\yahoo_answers_topics\\yahoo_answers_topics\\1.0.0\\0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa418686d78b4e22a05f05b0c05d2afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "yahoo = load_dataset('yahoo_answers_topics')\n",
    "banking = load_dataset('banking77')\n",
    "amazon = load_dataset('amazon_reviews_multi')\n",
    "financial = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "tweets = load_dataset('tweet_eval', 'sentiment')\n",
    "tweets_emotion = load_dataset('tweet_eval', 'emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(label):\n",
    "    label = re.sub(\"([a-z])([A-Z])\", \"\\\\1 \\\\2\", label)\n",
    "    label = label.replace(\"_\", \" \")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def sample_test_data(texts,  labels, size):\n",
    "    \n",
    "    data = list(zip(texts, labels))\n",
    "    data = [item for item in data if len(item[0].strip()) > 0]\n",
    "    \n",
    "    random.shuffle(data)\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "    \n",
    "    return texts[:size], labels[:size], texts[size:], labels[size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_test_texts, yahoo_test_labels, _, _ = sample_test_data(yahoo['test']['best_answer'], yahoo['test']['topic'], 1000)\n",
    "banking_test_texts, banking_test_labels, _, _ = sample_test_data(banking['test']['text'], banking['test']['label'], 1000)\n",
    "financial_test_texts, financial_test_labels, financial_train_texts, financial_train_labels = sample_test_data(financial['train']['sentence'], financial['train']['label'], 1000)\n",
    "tweets_test_texts, tweets_test_labels, _, _ = sample_test_data(tweets['test']['text'], tweets['test']['label'], 1000)\n",
    "tweets_emotion_test_texts, tweets_emotion_test_labels, _, _ = sample_test_data(tweets_emotion['test']['text'], tweets_emotion['test']['label'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"name\": \"yahoo_answers_topics\", \n",
    "        \"test_texts\": yahoo_test_texts, \n",
    "        \"test_labels\": yahoo_test_labels, \n",
    "        \"train_texts\": yahoo['train']['best_answer'], \n",
    "        \"train_labels\": yahoo['train']['topic'], \n",
    "        \"class_names\": [clean(label) for label in yahoo['test'].features['topic'].names]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"banking77\", \n",
    "        \"test_texts\": banking_test_texts,\n",
    "        \"test_labels\": banking_test_labels, \n",
    "        \"train_texts\": banking['train']['text'], \n",
    "        \"train_labels\": banking['train']['label'], \n",
    "        \"class_names\": [clean(label) for label in banking['test'].features['label'].names]\n",
    "    },    \n",
    "    {\n",
    "        \"name\": \"tweet_eval: emotion\", \n",
    "        \"test_texts\": tweets_emotion_test_texts, \n",
    "        \"test_labels\": tweets_emotion_test_labels, \n",
    "        \"train_texts\": tweets_emotion['train']['text'], \n",
    "        \"train_labels\": tweets_emotion['train']['label'], \n",
    "        \"class_names\": [clean(label) for label in tweets_emotion['test'].features['label'].names]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tweet_eval: sentiment\", \n",
    "        \"test_texts\": tweets_test_texts, \n",
    "        \"test_labels\": tweets_test_labels,\n",
    "        \"train_texts\": tweets['train']['text'], \n",
    "        \"train_labels\": tweets['train']['label'],\n",
    "        \"class_names\": [clean(label) for label in tweets['test'].features['label'].names]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"financial_phrasebank\", \n",
    "        \"test_texts\": financial_test_texts, \n",
    "        \"test_labels\": financial_test_labels, \n",
    "        \"train_texts\": financial_train_texts, \n",
    "        \"train_labels\": financial_train_labels, \n",
    "        \"class_names\":  [clean(label) for label in financial['train'].features['label'].names]\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-apollo",
   "metadata": {},
   "source": [
    "## The models\n",
    "\n",
    "We used three different zero-shot text classifiers in our tests: Flair’s `TARSClassifier`, and two Transformers models finetuned for NLI: `bart-large-mnli` and `roberta-large-mnli`. The graph below shows their accuracies on the five tasks. The results are as varied as the datasets, but one pattern is immediately clear: the best model always takes the NLI approach. For the Yahoo Answers topics, Bart gives the best accuracy (39.2%), followed by TARS and Roberta, which both obtain 27.5%. Although the banking task appears much more challenging at first sight, the NLI models perform even better here: they both classify over 41% of the test items correctly, leaving the TARS classifier far behind. On four-way tweet emotion classification, both Bart (73.6%) and Roberta (71.9%) perform surprisingly well and easily beat TARS (32.3%).\n",
    "\n",
    "The two polarity tasks deserve some additional explanation. Because our first evaluation run showed very low scores for TARS (accuracies below the random baseline of 33%), we took a closer look at the results and found that in most cases, TARS failed to predict a single label for the news sentences and tweets. To fix this, we performed a second run, where we made TARS return `neutral` for every sentence without a label. It is those scores you see below.\n",
    "For both polarity tasks, Roberta gives the best results, with 54.0% accuracy for the tweets and 58.8% for the\n",
    "financial news sentences. TARS and Bart obtain a similar result on the tweets, with 48.2% and 49.0% accuracy, respectively. On the financial news sentences, TARS is better, with 51.7% against 38.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models.text_classification_model import TARSClassifier\n",
    "from flair.data import Sentence\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_flair(dataset, default_name='neutral'):\n",
    "\n",
    "    classifier = TARSClassifier.load('tars-base')\n",
    "        \n",
    "    total, correct = 0, 0\n",
    "    for item, gold_label_idx in tqdm(zip(dataset[\"test_texts\"], dataset[\"test_labels\"]), total = len(dataset[\"test_texts\"])):\n",
    "        sentence = Sentence(item)\n",
    "        classifier.predict_zero_shot(sentence, dataset[\"class_names\"])\n",
    "        sorted_labels = sorted(sentence.to_dict()['labels'], key=lambda k: k['confidence'], reverse=True)\n",
    "        gold_label = dataset[\"class_names\"][gold_label_idx]\n",
    "        if len(sorted_labels) > 0:\n",
    "            predicted_label = sorted_labels[0]['value']\n",
    "        else:\n",
    "            predicted_label = default_name\n",
    "        if predicted_label == gold_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "def evaluate_huggingface(dataset, template=None, model='base'):\n",
    "\n",
    "    if model == 'base':\n",
    "        classifier = pipeline(\"zero-shot-classification\", device=0)\n",
    "    else:\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\", device=0)\n",
    "    \n",
    "    correct = 0\n",
    "    predictions, gold_labels = [], []\n",
    "    for text, gold_label_idx in tqdm(zip(dataset[\"test_texts\"], dataset[\"test_labels\"]), total=len(dataset[\"test_texts\"])):\n",
    "\n",
    "        if template is not None:\n",
    "            result = classifier(text, dataset[\"class_names\"], multi_label=False, template=template)\n",
    "        else:\n",
    "            result = classifier(text, dataset[\"class_names\"], multi_label=False)\n",
    "        predicted_label = result['labels'][0]\n",
    "        \n",
    "        gold_label = dataset[\"class_names\"][gold_label_idx]\n",
    "        \n",
    "        predictions.append(predicted_label)\n",
    "        gold_labels.append(gold_label)\n",
    "        \n",
    "        if predicted_label == gold_label:\n",
    "            correct += 1\n",
    "            \n",
    "    accuracy = correct/len(predictions)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-murder",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for dataset in data:\n",
    "    print(dataset['name'])\n",
    "    results[dataset['name']] = {}\n",
    "    \n",
    "    flair_acc = evaluate_flair(dataset)\n",
    "    results[dataset['name']]['Flair TARS'] = flair_acc\n",
    "    print(\"Flair:\", flair_acc)\n",
    "    \n",
    "    huggingface_acc = evaluate_huggingface(dataset)\n",
    "    results[dataset['name']]['Transformers Bart'] = huggingface_acc\n",
    "    print(\"Huggingface Bart\", huggingface_acc)\n",
    "\n",
    "    huggingface_acc_roberta = evaluate_huggingface(dataset, model='roberta')\n",
    "    results[dataset['name']]['Transformers Roberta'] = huggingface_acc_roberta\n",
    "    print(\"Huggingface Roberta\", huggingface_acc_roberta)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df = df.transpose()\n",
    "df.plot(kind='bar', figsize=(12,7), colormap='copper', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-japanese",
   "metadata": {},
   "source": [
    "## The impact of class names\n",
    "\n",
    "In traditional supervised learning, the actual names of the labels do not have any impact on the performance of the model. You’re free to call your classes whatever you want &mdash; `positive`, `politics` or `aardvark`, it makes no difference at all. In zero-shot text classification, these names suddenly become important. Both the NLI and TARS classifiers add the label itself to the input of the model, so that the class names have the power to change the predictions. In general, the more semantic information about the class they contain, and the more similar they are to the type of data that the model was finetuned on, the better we can expect the classifier to perform.\n",
    "\n",
    "Let’s take polarity classification as an example. While `positive`, `neutral` and `negative` are the traditional class names for this task, they may not be optimal for a zero-shot approach. To test this out, we experimented\n",
    "with two alternative sets of names for the financial news data: `good news`, `neutral news` and `bad news` on the one hand, and `happy news`, `neutral news` and `unhappy news` on the other. As you can see in the figure below, this has a very positive effect on the accuracy of the classifiers. Both TARS (62.0%) and Bart (61.9%) now perform better than the original Roberta, although Bart only does so with the happy/unhappy class names. Roberta itself jumps another 13%, to an accuracy of over 73% with both alternative sets of names. Zero-shot classifiers may reduce the need for labeling, but they do introduce the necessary task of searching for good class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[3][\"class_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_results = {}\n",
    "alternative_results['positive/negative'] = results['financial_phrasebank']\n",
    "\n",
    "alternative_financial_news = {\n",
    "    \"name\": \"good/bad\", \n",
    "    \"test_texts\": financial_test_texts, \n",
    "    \"test_labels\": financial_test_labels, \n",
    "    \"class_names\":  [\"bad news\", \"neutral news\", \"good news\"]\n",
    "    }\n",
    "\n",
    "alternative_results[alternative_financial_news['name']] = {}\n",
    "flair_acc = evaluate_flair(alternative_financial_news, default_name='neutral news')\n",
    "alternative_results[alternative_financial_news['name']]['Flair TARS'] = flair_acc\n",
    "print(\"Flair:\", flair_acc)\n",
    "\n",
    "huggingface_acc = evaluate_huggingface(alternative_financial_news)\n",
    "alternative_results[alternative_financial_news['name']]['Transformers Bart'] = huggingface_acc\n",
    "print(\"Huggingface Bart\", huggingface_acc)\n",
    "\n",
    "huggingface_acc_roberta = evaluate_huggingface(alternative_financial_news, model='roberta')\n",
    "alternative_results[alternative_financial_news['name']]['Transformers Roberta'] = huggingface_acc_roberta\n",
    "print(\"Huggingface Roberta\", huggingface_acc_roberta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_financial_news = {\n",
    "    \"name\": \"happy/unhappy\", \n",
    "    \"test_texts\": financial_test_texts, \n",
    "    \"test_labels\": financial_test_labels, \n",
    "    \"class_names\":  [\"unhappy news\", \"neutral news\", \"happy news\"]\n",
    "    }\n",
    "\n",
    "alternative_results[alternative_financial_news['name']] = {}\n",
    "flair_acc = evaluate_flair(alternative_financial_news, default_name='neutral news')\n",
    "alternative_results[alternative_financial_news['name']]['Flair TARS'] = flair_acc\n",
    "print(\"Flair:\", flair_acc)\n",
    "\n",
    "huggingface_acc = evaluate_huggingface(alternative_financial_news)\n",
    "alternative_results[alternative_financial_news['name']]['Transformers Bart'] = huggingface_acc\n",
    "print(\"Huggingface Bart\", huggingface_acc)\n",
    "\n",
    "huggingface_acc_roberta = evaluate_huggingface(alternative_financial_news, model='roberta')\n",
    "alternative_results[alternative_financial_news['name']]['Transformers Roberta'] = huggingface_acc_roberta\n",
    "print(\"Huggingface Roberta\", huggingface_acc_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(alternative_results)\n",
    "df.plot(kind='bar', figsize=(12,7), colormap='copper', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-olive",
   "metadata": {},
   "source": [
    "## Few-shot learning\n",
    "\n",
    "As we saw with polarity classification, the TARS classifier tends to suffer from low recall. In tasks with a default class, such as `neutral`, this label can serve as a fallback option, but in other cases we need a different solution.\n",
    "Luckily, Flairs has made it easy to finetune TARS on a handful of training examples. Let’s see what happens if we give the model one example of what we mean by each class, and finetune it on this small training set. Because the performance of the final model will depend on what training instances we pick, we repeat this process ten times and always select random examples from the training corpus.\n",
    "\n",
    "The figure below shows that the TARS classifier benefits greatly from this few-shot learning procedure. The impact is clearest for the three tasks without a default class: TARS’s accuracy jumps significantly, and in two out of three cases it becomes competitive with the best NLI model. Interestingly, this even happens for the emotion dataset, where we’ve used just four examples as our training set. For the polarity tasks, the benefit is less clear, as we already fixed the recall problem by introducing a default class, and only worked with three labeled examples for few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import SentenceDataset\n",
    "\n",
    "def sample_training_corpus(dataset, seed):\n",
    "\n",
    "    random.seed(seed)\n",
    "    \n",
    "    seen_labels = set()\n",
    "    sentences = []\n",
    "\n",
    "    train_set = list(zip(dataset['train_texts'], dataset['train_labels']))\n",
    "    random.shuffle(train_set)\n",
    "    \n",
    "    for text, label in train_set:\n",
    "        topic = dataset['class_names'][label]\n",
    "        if topic not in seen_labels:\n",
    "            sentences.append(Sentence(text).add_label(\"_or_\".join(dataset['class_names']), topic))\n",
    "            seen_labels.add(topic)\n",
    "        if len(seen_labels) == len(dataset['class_names']):\n",
    "            break\n",
    "\n",
    "    train = SentenceDataset(sentences)\n",
    "    corpus = Corpus(train=train)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "def train(corpus, dataset):\n",
    "\n",
    "    tars = TARSClassifier.load('tars-base')\n",
    "    tars.add_and_switch_to_new_task(dataset['name'], label_dictionary=corpus.make_label_dictionary())\n",
    "\n",
    "    trainer = ModelTrainer(tars, corpus)\n",
    "\n",
    "    trainer.train(base_path='/tmp/' + dataset['name'], # path to store the model artifacts\n",
    "              learning_rate=0.02, # use very small learning rate\n",
    "              mini_batch_size=1, # small mini-batch size since corpus is tiny\n",
    "              max_epochs=10, # terminate after 10 epochs\n",
    "              train_with_dev=False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, default_name='neutral'):\n",
    "    tars = TARSClassifier.load(f'/tmp/{dataset[\"name\"]}/final-model.pt')\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    for item, gold_label_idx in tqdm(zip(dataset[\"test_texts\"], dataset[\"test_labels\"]), total = len(dataset[\"test_texts\"])):\n",
    "        sentence = Sentence(item)\n",
    "        tars.predict(sentence)\n",
    "        sorted_labels = sorted(sentence.to_dict()['labels'], key=lambda k: k['confidence'], reverse=True)\n",
    "        \n",
    "        gold_label = dataset[\"class_names\"][gold_label_idx]\n",
    "        if len(sorted_labels) > 0:\n",
    "            predicted_label = sorted_labels[0]['value']\n",
    "        else:\n",
    "            predicted_label = default_name                \n",
    "\n",
    "        if predicted_label == gold_label:\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_fewshot = {}\n",
    "\n",
    "for dataset in data[3:]:\n",
    "    results_fewshot[dataset['name']] = []\n",
    "    for seed in range(10):\n",
    "\n",
    "        corpus = sample_training_corpus(dataset, seed)\n",
    "        train(corpus, dataset)\n",
    "        acc = evaluate(dataset)\n",
    "        results_fewshot[dataset['name']].append(acc)\n",
    "\n",
    "        print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.DataFrame(results_fewshot)\n",
    "#sns.set(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Show each distribution with both violins and points\n",
    "sns.violinplot(data=df, palette=\"Set3\", inner=\"points\",bw =.2, cut=2,linewidth=2, orient=\"v\")\n",
    "\n",
    "sns.despine(left=True)\n",
    "\n",
    "#f.suptitle(\"TARS Few-Shot Learning\", fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel(\"Accuracy\", size = 12, alpha=0.7)\n",
    "ax.set_ylim([0, 1])\n",
    "#ax.set_ylabel(\"Model\",size = 14, alpha=0.7)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(11) \n",
    "\n",
    "    \n",
    "for i, result in enumerate(results):\n",
    "    ax.plot(i, results[data[i]['name']]['Flair TARS'], 'rx', markersize=12)\n",
    "    ax.plot(i, results[data[i]['name']]['Transformers Bart'], 'r+', markersize=12)\n",
    "    ax.plot(i, results[data[i]['name']]['Transformers Roberta'], 'ro', markersize=12)\n",
    "    \n",
    "ax.legend(['Flair TARS zero-shot', 'Transformers Bart', 'Transformers Roberta'], fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-optics",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Without a doubt, zero-shot learning is an extraordinary application of transfer learning. Zero-shot classifiers predict the class of a text without having seen a single labeled example, and in some cases do so with a higher accuracy than supervised models that have been trained on hundreds of labeled training items. Their success is far from guaranteed &mdash; it depends on the particular task and a careful selection of class names &mdash; but in the right circumstances, these models can get you a long way towards accurate text classification.\n",
    "\n",
    "At the same time, the open-source zero-shot classifiers we tested out are no magic solutions, as it’s unlikely they are going to give you optimal performance on a specialized NLP task. For such applications, manually labeling a large number of examples, for example with a tool like <a href='https://www.tagalog.ai/'>Tagalog</a> still gives you the best chance of success. Even in those cases, however, zero-shot classification can prove useful, for example as a way to speed up manual labeling by suggesting potentially relevant labels to the annotators. It’s clear zero-shot and few-shot classification is here to stay, and can be a useful tool in any NLPer’s toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
