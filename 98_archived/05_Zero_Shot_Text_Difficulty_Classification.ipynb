{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d8f5fb",
   "metadata": {},
   "source": [
    "## Zero-Shot Text Difficulty Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c875e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Zero-shot and few-shot NLP models are used to handle NLP given the limited dataset. This note is trying to explore those algorithms to improve the accuracy of text classification in terms of text difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0908d",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769c3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model can be loaded with the zero-shot-classification pipeline like so:\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9277c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'When Japan was added back to the F1 schedule ten years later , it went to Suzuka instead .',\n",
       "  'labels': ['1', '0'],\n",
       "  'scores': [0.587661623954773, 0.41233840584754944]},\n",
       " {'sequence': 'Before Persephone was released to Hermes , who had been sent to retrieve her , Hades tricked her into eating pomegranate seeds , -LRB- six or three according to the telling -RRB- which forced her to return to the underworld for a period each year .',\n",
       "  'labels': ['1', '0'],\n",
       "  'scores': [0.6243764758110046, 0.37562352418899536]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this pipeline to classify sequences into any of the class names you specify.\n",
    "sequence_to_classify = [\"When Japan was added back to the F1 schedule ten years later , it went to Suzuka instead .\",\n",
    "                       \"Before Persephone was released to Hermes , who had been sent to retrieve her , Hades tricked her into eating pomegranate seeds , -LRB- six or three according to the telling -RRB- which forced her to return to the underworld for a period each year .\"]\n",
    "candidate_labels = ['0','1']\n",
    "\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
    "#{'labels': ['travel', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n",
    "# 'sequence': 'one day I will see the world'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931593e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
       " 'scores': [0.994511067867279,\n",
       "  0.9383885264396667,\n",
       "  0.005706145893782377,\n",
       "  0.0018192846328020096]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label=True)\n",
    "#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9945111274719238,\n",
    "#  0.9383890628814697,\n",
    "#  0.0057061901316046715,\n",
    "#  0.0018193122232332826],\n",
    "# 'sequence': 'one day I will see the world'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3b6d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 371340\n",
      "Dev size: 41260\n",
      "Test size: 4168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_path=\"./01_data/WikiLarge_Train.csv\"\n",
    "train_data=pd.read_csv(train_data_path)\n",
    "\n",
    "size=round(len(train_data)*1)\n",
    "r_train=train_data.sample(n=size)\n",
    "texts=list(r_train[\"original_text\"])\n",
    "labels=list(r_train[\"label\"])\n",
    "    \n",
    "rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.01, random_state=1)\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n",
    "\n",
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Dev size:\", len(dev_texts))\n",
    "print(\"Test size:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a0c88c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dc997ee7344a819ffc589fff037889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "candidate_labels = ['0','1']\n",
    "\n",
    "epoches=20\n",
    "rests=len(test_texts)%epoches\n",
    "batches=int(len(test_texts)/epoches)\n",
    "\n",
    "for i in range(epoches):  \n",
    "    if i==0:\n",
    "        test_result=classifier(test_texts[:batches],candidate_labels, multi_label=False)\n",
    "    else:\n",
    "        if i==epoches-1:\n",
    "            test_result_tmp=classifier(test_texts[i*batches:(i+1)*batches+rests],candidate_labels, multi_label=False)\n",
    "        else:\n",
    "             test_result_tmp=classifier(test_texts[i*batches:(i+1)*batches],candidate_labels, multi_label=False)\n",
    "        test_result=test_result+test_result_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "866cb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_result=pd.DataFrame.from_dict(test_result)\n",
    "df_test_result['labels']=df_test_result['labels'].apply(lambda x: int(x[0]))\n",
    "df_test_result['scores']=df_test_result['scores'].apply(lambda x: float(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5ff728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance: (0.4882437619961612, 0.4882437619961612, 0.4882437619961612, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.03      0.06      2096\n",
      "           1       0.49      0.95      0.65      2072\n",
      "\n",
      "    accuracy                           0.49      4168\n",
      "   macro avg       0.44      0.49      0.35      4168\n",
      "weighted avg       0.44      0.49      0.35      4168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "test_correct=test_labels\n",
    "test_predicted=df_test_result['labels']\n",
    "print(\"Test performance:\", precision_recall_fscore_support(test_correct, test_predicted, average=\"micro\"))\n",
    "\n",
    "bert_accuracy = np.mean(test_predicted == test_correct)\n",
    "\n",
    "print(classification_report(test_correct, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda6031",
   "metadata": {},
   "source": [
    "#### Manual Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f002ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose sequence as a NLI premise and label as a hypothesis\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb096ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2335: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nli_model.to(device)\n",
    "\n",
    "premise = sequence_to_classify[0]\n",
    "label=\"1\"\n",
    "hypothesis = f'This example is {label}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')\n",
    "logits = nli_model(x.to(device))[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f94c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5768], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_label_is_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088a5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
