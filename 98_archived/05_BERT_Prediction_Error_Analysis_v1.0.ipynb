{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3c1e9a",
   "metadata": {},
   "source": [
    "# BERT Prediction Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64fc66",
   "metadata": {},
   "source": [
    "As the result of text classification by 3 types of BERT models ( BERT_BASED_UNCASED, BERT_LARGE_UNCASED, DISTILBERT), it is found that:\n",
    " - The smaller the model is, the higher the accuracies. Distilbert achieved 77.8% in contrast that Bert_large_uncased only achieved 70& around.\n",
    "\n",
    "In order to improve the accuracy level, we will perform the further study on those records whose prediction result are wrong, and see whether there are some ways.\n",
    "\n",
    "Given Bert_large_uncased does not achieved high accuracy, in order to save the potential efforts, we take the remaining 2 models for the error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7d23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Disable 3 types of warning\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=(FutureWarning))\n",
    "warnings.filterwarnings(\"ignore\",category=(RuntimeWarning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb206c6e",
   "metadata": {},
   "source": [
    "##### import error data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511de07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import bert test error data\n",
    "error_data_path=\"./tmp/test_error_2022-10-09.csv\"\n",
    "error_data=pd.read_csv(error_data_path)\n",
    "\n",
    "# import bert full test data\n",
    "test_data_path=\"./tmp/test_full_2022-10-09.csv\"\n",
    "test_data=pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767783f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated records: 4.16%\n"
     ]
    }
   ],
   "source": [
    "test_data['duplicated']=test_data.duplicated(subset=['original_text'])\n",
    "test_data_unique=test_data[test_data['duplicated']==False]\n",
    "print(\"Duplicated records: %.2f%%\" %(100*(len(test_data)-len(test_data_unique))/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66e4835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Antilopine Kangaroo -LRB- Macropus antilop...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pepper spray , for blinding people</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2 Maccabees was written in Koine Greek , proba...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Dukas 's use of rhythm and his skill as an orc...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The island of Ireland was historically divided...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41672</th>\n",
       "      <td>41672</td>\n",
       "      <td>Soon afterward a new marketplace was opened , ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41673</th>\n",
       "      <td>41673</td>\n",
       "      <td>The 1960 Formula One season featured the eleve...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41674</th>\n",
       "      <td>41674</td>\n",
       "      <td>In a few special cases , an unreleased album m...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41675</th>\n",
       "      <td>41675</td>\n",
       "      <td>Aside from this central belief , its ideology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41676</th>\n",
       "      <td>41676</td>\n",
       "      <td>The Fraser Institute , however , criticized th...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39942 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                      original_text  label  \\\n",
       "0               0  The Antilopine Kangaroo -LRB- Macropus antilop...      1   \n",
       "1               1                 pepper spray , for blinding people      0   \n",
       "2               2  2 Maccabees was written in Koine Greek , proba...      0   \n",
       "3               3  Dukas 's use of rhythm and his skill as an orc...      1   \n",
       "4               4  The island of Ireland was historically divided...      1   \n",
       "...           ...                                                ...    ...   \n",
       "41672       41672  Soon afterward a new marketplace was opened , ...      1   \n",
       "41673       41673  The 1960 Formula One season featured the eleve...      1   \n",
       "41674       41674  In a few special cases , an unreleased album m...      0   \n",
       "41675       41675  Aside from this central belief , its ideology ...      0   \n",
       "41676       41676  The Fraser Institute , however , criticized th...      0   \n",
       "\n",
       "       duplicated  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  \n",
       "...           ...  \n",
       "41672       False  \n",
       "41673       False  \n",
       "41674       False  \n",
       "41675       False  \n",
       "41676       False  \n",
       "\n",
       "[39942 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8e8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records with different labels: 0.08%\n"
     ]
    }
   ],
   "source": [
    "# look for the records with different labels \n",
    "data=test_data\n",
    "df_by=pd.DataFrame(data.groupby(['original_text','label']).count().reset_index()[[\"original_text\",\"label\"]])\n",
    "df_by=df_by.groupby(by='original_text').count().sort_values('label',ascending=False).reset_index()\n",
    "diff_labels=df_by[df_by['label']>1]\n",
    "print(\"Records with different labels: %.2f%%\" %(100*len(diff_labels)/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4cafe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pattern of reducing the contestants down w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-- -RRB-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berlin 1981 , ISBN 3-540-10857-2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Serbia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by Philip K. Dick .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rolling Stone link</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New Mexico and Iowa voted Democratic in 2000 ,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>After he retired from music he worked for a pr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Holy Grail or '' Graal '' in older forms</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Photometry of Irregular Satellites of Uranus a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Asclepias incarnata</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A total of three cassette compilations were re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ISBN 0896084272 .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>An even more unusual move is the en passant ca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1 × Second-Team All-Pro selection -LRB-/O2003/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>One woman shoe .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It may be written in Arabic as or .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Both intelligent patrons of literature and art...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>He also has written scientific works about Leo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>For official forecasts see : the NHC 's public...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>These dates conventionally delimit the period ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ananke</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>People</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>London .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>It produces the cork cambium , another seconda...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-LRB-/O1979/O-RRB- .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Scots Confession -LRB-/O1560/O-RRB- ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>conf_champs = 3 -LRB- 1978 , 1979 , 1996 -RRB-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        original_text  label\n",
       "0   The pattern of reducing the contestants down w...      2\n",
       "1                                            -- -RRB-      2\n",
       "2                    Berlin 1981 , ISBN 3-540-10857-2      2\n",
       "3                                              Serbia      2\n",
       "4                                 by Philip K. Dick .      2\n",
       "5                                            Pakistan      2\n",
       "6                                                   .      2\n",
       "7                                  Rolling Stone link      2\n",
       "8   New Mexico and Iowa voted Democratic in 2000 ,...      2\n",
       "9   After he retired from music he worked for a pr...      2\n",
       "10           Holy Grail or '' Graal '' in older forms      2\n",
       "11  Photometry of Irregular Satellites of Uranus a...      2\n",
       "12                                Asclepias incarnata      2\n",
       "13  A total of three cassette compilations were re...      2\n",
       "14                                  ISBN 0896084272 .      2\n",
       "15  An even more unusual move is the en passant ca...      2\n",
       "16  1 × Second-Team All-Pro selection -LRB-/O2003/...      2\n",
       "17                                   One woman shoe .      2\n",
       "18                                                jpg      2\n",
       "19                It may be written in Arabic as or .      2\n",
       "20  Both intelligent patrons of literature and art...      2\n",
       "21  He also has written scientific works about Leo...      2\n",
       "22  For official forecasts see : the NHC 's public...      2\n",
       "23  These dates conventionally delimit the period ...      2\n",
       "24                                             Ananke      2\n",
       "25                                             People      2\n",
       "26                                           London .      2\n",
       "27  It produces the cork cambium , another seconda...      2\n",
       "28                               -LRB-/O1979/O-RRB- .      2\n",
       "29              Scots Confession -LRB-/O1560/O-RRB- ,      2\n",
       "30                                             #NAME?      2\n",
       "31     conf_champs = 3 -LRB- 1978 , 1979 , 1996 -RRB-      2\n",
       "32                                       South Africa      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3d15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indentify double labels in data\n",
    "test_data_unique=test_data_unique.merge(diff_labels,how=\"left\",left_on=\"original_text\",right_on=\"original_text\")\n",
    "test_data_unique['label_y']=test_data_unique['label_y'].apply(lambda x: '0' if pd.isnull(x) else '1') # 0 means 1 label, 1 means 2 labels\n",
    "test_data_unique=pd.DataFrame(test_data_unique[['original_text','label_x','label_y','duplicated']])\n",
    "test_data_unique.columns=['original_text','label','dulabel','duplicated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25546482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>dulabel</th>\n",
       "      <th>duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Antilopine Kangaroo -LRB- Macropus antilop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pepper spray , for blinding people</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 Maccabees was written in Koine Greek , proba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dukas 's use of rhythm and his skill as an orc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The island of Ireland was historically divided...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39937</th>\n",
       "      <td>Soon afterward a new marketplace was opened , ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39938</th>\n",
       "      <td>The 1960 Formula One season featured the eleve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39939</th>\n",
       "      <td>In a few special cases , an unreleased album m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39940</th>\n",
       "      <td>Aside from this central belief , its ideology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39941</th>\n",
       "      <td>The Fraser Institute , however , criticized th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39942 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           original_text  label dulabel  \\\n",
       "0      The Antilopine Kangaroo -LRB- Macropus antilop...      1       0   \n",
       "1                     pepper spray , for blinding people      0       0   \n",
       "2      2 Maccabees was written in Koine Greek , proba...      0       0   \n",
       "3      Dukas 's use of rhythm and his skill as an orc...      1       0   \n",
       "4      The island of Ireland was historically divided...      1       0   \n",
       "...                                                  ...    ...     ...   \n",
       "39937  Soon afterward a new marketplace was opened , ...      1       0   \n",
       "39938  The 1960 Formula One season featured the eleve...      1       0   \n",
       "39939  In a few special cases , an unreleased album m...      0       0   \n",
       "39940  Aside from this central belief , its ideology ...      0       0   \n",
       "39941  The Fraser Institute , however , criticized th...      0       0   \n",
       "\n",
       "       duplicated  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  \n",
       "...           ...  \n",
       "39937       False  \n",
       "39938       False  \n",
       "39939       False  \n",
       "39940       False  \n",
       "39941       False  \n",
       "\n",
       "[39942 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a514b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data['duplicated']=error_data.duplicated(subset=['original_text'])\n",
    "error_data_unique=error_data[error_data['duplicated']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8fe47f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Second-wave feminism , also sometimes called w...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In My First Signs , it was not originally plan...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In an episode where Raw returned to the Manhat...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In mathematics , hyperbolic geometry is a non-...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Events The largest horse show in Norway , the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9585</th>\n",
       "      <td>9585</td>\n",
       "      <td>is an electronic music group created by German...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9586</th>\n",
       "      <td>9586</td>\n",
       "      <td>They combine elements of black metal , death m...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9587</th>\n",
       "      <td>9587</td>\n",
       "      <td>In a few special cases , an unreleased album m...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9588</th>\n",
       "      <td>9588</td>\n",
       "      <td>Aside from this central belief , its ideology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>9589</td>\n",
       "      <td>The Fraser Institute , however , criticized th...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9438 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      original_text  label  \\\n",
       "0              0  Second-wave feminism , also sometimes called w...      1   \n",
       "1              1  In My First Signs , it was not originally plan...      1   \n",
       "2              2  In an episode where Raw returned to the Manhat...      0   \n",
       "3              3  In mathematics , hyperbolic geometry is a non-...      0   \n",
       "4              4  Events The largest horse show in Norway , the ...      1   \n",
       "...          ...                                                ...    ...   \n",
       "9585        9585  is an electronic music group created by German...      0   \n",
       "9586        9586  They combine elements of black metal , death m...      1   \n",
       "9587        9587  In a few special cases , an unreleased album m...      0   \n",
       "9588        9588  Aside from this central belief , its ideology ...      0   \n",
       "9589        9589  The Fraser Institute , however , criticized th...      0   \n",
       "\n",
       "      duplicated  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  \n",
       "...          ...  \n",
       "9585       False  \n",
       "9586       False  \n",
       "9587       False  \n",
       "9588       False  \n",
       "9589       False  \n",
       "\n",
       "[9438 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_data_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f248fba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>correct</th>\n",
       "      <th>duplicated</th>\n",
       "      <th>dulabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Antilopine Kangaroo -LRB- Macropus antilop...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pepper spray , for blinding people</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 Maccabees was written in Koine Greek , proba...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dukas 's use of rhythm and his skill as an orc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The island of Ireland was historically divided...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39937</th>\n",
       "      <td>Soon afterward a new marketplace was opened , ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39938</th>\n",
       "      <td>The 1960 Formula One season featured the eleve...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39939</th>\n",
       "      <td>In a few special cases , an unreleased album m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39940</th>\n",
       "      <td>Aside from this central belief , its ideology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39941</th>\n",
       "      <td>The Fraser Institute , however , criticized th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39942 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           original_text  label correct  \\\n",
       "0      The Antilopine Kangaroo -LRB- Macropus antilop...      1       1   \n",
       "1                     pepper spray , for blinding people      0       1   \n",
       "2      2 Maccabees was written in Koine Greek , proba...      0       1   \n",
       "3      Dukas 's use of rhythm and his skill as an orc...      1       1   \n",
       "4      The island of Ireland was historically divided...      1       1   \n",
       "...                                                  ...    ...     ...   \n",
       "39937  Soon afterward a new marketplace was opened , ...      1       1   \n",
       "39938  The 1960 Formula One season featured the eleve...      1       1   \n",
       "39939  In a few special cases , an unreleased album m...      0       0   \n",
       "39940  Aside from this central belief , its ideology ...      0       0   \n",
       "39941  The Fraser Institute , however , criticized th...      0       0   \n",
       "\n",
       "       duplicated dulabel  \n",
       "0           False       0  \n",
       "1           False       0  \n",
       "2           False       0  \n",
       "3           False       0  \n",
       "4           False       0  \n",
       "...           ...     ...  \n",
       "39937       False       0  \n",
       "39938       False       0  \n",
       "39939       False       0  \n",
       "39940       False       0  \n",
       "39941       False       0  \n",
       "\n",
       "[39942 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe with corrected \n",
    "test_full=test_data_unique.merge(error_data_unique,how=\"left\",left_on=\"original_text\",right_on=\"original_text\")\n",
    "test_full['label_y']=test_full['label_y'].apply(lambda x: '1' if pd.isnull(x) else '0')\n",
    "test_full['duplicated']=test_full.duplicated(subset=['original_text'])\n",
    "test_full_unique=test_full[test_full['duplicated']==False]\n",
    "test_full=pd.DataFrame(test_full[['original_text','label_x','label_y','duplicated','dulabel']])\n",
    "test_full.columns=['original_text','label','correct','duplicated','dulabel']\n",
    "test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6093b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full.to_csv(\"./00_error_analysis/test_full.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "409061c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of original text length in words')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TRAIN SET \n",
    "test_full['original_text'].apply(lambda x: len(x.split())).plot(kind='hist');\n",
    "plt.yscale('log');\n",
    "plt.title('Distribution of original text length in words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554f9032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9590.000000\n",
       "mean       21.934619\n",
       "std        11.607622\n",
       "min         1.000000\n",
       "25%        14.000000\n",
       "50%        20.000000\n",
       "75%        28.000000\n",
       "max        80.000000\n",
       "Name: original_text, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_data['original_text'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad857e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Second-wave feminism , also sometimes called w...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In My First Signs , it was not originally plan...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In an episode where Raw returned to the Manhat...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In mathematics , hyperbolic geometry is a non-...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Events The largest horse show in Norway , the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9585</th>\n",
       "      <td>9585</td>\n",
       "      <td>is an electronic music group created by German...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9586</th>\n",
       "      <td>9586</td>\n",
       "      <td>They combine elements of black metal , death m...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9587</th>\n",
       "      <td>9587</td>\n",
       "      <td>In a few special cases , an unreleased album m...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9588</th>\n",
       "      <td>9588</td>\n",
       "      <td>Aside from this central belief , its ideology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>9589</td>\n",
       "      <td>The Fraser Institute , however , criticized th...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9590 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      original_text  label  \\\n",
       "0              0  Second-wave feminism , also sometimes called w...      1   \n",
       "1              1  In My First Signs , it was not originally plan...      1   \n",
       "2              2  In an episode where Raw returned to the Manhat...      0   \n",
       "3              3  In mathematics , hyperbolic geometry is a non-...      0   \n",
       "4              4  Events The largest horse show in Norway , the ...      1   \n",
       "...          ...                                                ...    ...   \n",
       "9585        9585  is an electronic music group created by German...      0   \n",
       "9586        9586  They combine elements of black metal , death m...      1   \n",
       "9587        9587  In a few special cases , an unreleased album m...      0   \n",
       "9588        9588  Aside from this central belief , its ideology ...      0   \n",
       "9589        9589  The Fraser Institute , however , criticized th...      0   \n",
       "\n",
       "      duplicated  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  \n",
       "...          ...  \n",
       "9585       False  \n",
       "9586       False  \n",
       "9587       False  \n",
       "9588       False  \n",
       "9589       False  \n",
       "\n",
       "[9590 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ece0e",
   "metadata": {},
   "source": [
    "### Rerun the prediction to confirm the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "673bf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1e596fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#MAX_SEQ_LENGTH=100\n",
    "MAX_SEQ_LENGTH=80\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label2idx[label]\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "\n",
    "        \n",
    "    return input_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff48147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    #dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    # dataloader tuning in https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "   \n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size,num_workers=2,pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33b1ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #tmp_eval_loss, logits = model(input_ids, attention_mask=input_mask,\n",
    "            #                              token_type_ids=segment_ids, labels=label_ids)[:2]\n",
    "            tmp_eval_loss, logits = model(input_ids, attention_mask=input_mask,\n",
    "                                         labels=label_ids)[:2]  # for distilbert\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "        \n",
    "    return eval_loss, correct_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd228c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification,DistilBertForSequenceClassification\n",
    "from transformers import BertTokenizer,DistilBertTokenizer\n",
    "import os\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Evaluate the dataset based on trained distilbert model\n",
    "def data_evaluation(texts,labels,OUTPUT_DIR = \"./tmp/\", MODEL_FILE_NAME = \"pytorch_model_best.bin\"):\n",
    "    # Convert test data of submission to features\n",
    "    target_names = list(set(labels))\n",
    "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "    \n",
    "    # Enable GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Select bert model\n",
    "    BERT_MODEL = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL)\n",
    "    \n",
    "    # Using trained model\n",
    "    model_state_dict = torch.load(os.path.join(OUTPUT_DIR, MODEL_FILE_NAME), map_location=lambda storage, loc: storage)\n",
    "    model=DistilBertForSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels = len(target_names),\n",
    "                                                              ignore_mismatched_sizes=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert text and labels to embeddings \n",
    "    features = convert_examples_to_inputs(texts, labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "    dataloader = get_data_loader(features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Predict the result, and discard the evaluatoin result, only take the prediction result.\n",
    "    _, correct, predicted = evaluate(model, dataloader)\n",
    "    print(\"Errors performance:\", precision_recall_fscore_support(correct, predicted, average=\"micro\"))\n",
    "\n",
    "    #bert_accuracy = np.mean(predicted == correct)\n",
    "    \n",
    "    #print(bert_accuracy)\n",
    "    print(classification_report(correct, predicted))\n",
    "\n",
    "    return correct,predicted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f479b1",
   "metadata": {},
   "source": [
    "#### Verify the vocabulary and classifier on single record level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a38956",
   "metadata": {},
   "outputs": [],
   "source": [
    "    OUTPUT_DIR = \"./tmp/\"\n",
    "    MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "    target_names = list(set(test_full['label']))\n",
    "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "    \n",
    "    # Enable GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Select bert model\n",
    "    BERT_MODEL = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL)\n",
    "    \n",
    "    # Using trained model\n",
    "    model_state_dict = torch.load(os.path.join(OUTPUT_DIR, MODEL_FILE_NAME), map_location=lambda storage, loc: storage)\n",
    "    model=DistilBertForSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels = len(target_names),\n",
    "                                                              ignore_mismatched_sizes=True,\n",
    "                                                             output_attentions = False,\n",
    "                                                             output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab297783",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers#####################################################  \n",
    "# credit for https://www.kaggle.com/code/kyakovlev/preprocessing-bert-public/notebook\n",
    "######################################################################################## \n",
    "## Multiprocessing Run.\n",
    "# :df - DataFrame to split                      # type: pandas DataFrame\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "def df_parallelize_run(df, func):\n",
    "    num_partitions, num_cores = 16, psutil.cpu_count()  # number of partitions and cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "    \n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return sorted(vocab.items(), key=lambda kv: kv[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3637ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Wrong label\n",
      "Unknown words: 19752 | Known words: 9927\n"
     ]
    }
   ],
   "source": [
    "data_0 = test_full[test_full[\"correct\"]=='0']['original_text']\n",
    "local_vocab = tokenizer.vocab.keys()\n",
    "data_0 = data_0.astype(str)\n",
    "print('#' *20 ,'Wrong label'); check_vocab(data_0, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff2baae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Correct label\n",
      "Unknown words: 45414 | Known words: 13470\n"
     ]
    }
   ],
   "source": [
    "data_1 = test_full[test_full[\"correct\"]=='1']['original_text']\n",
    "data_1 = data_1.astype(str)\n",
    "print('#' *20 ,'Correct label'); check_vocab(data_1, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "906fb1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 46303),\n",
       " (',', 46005),\n",
       " ('.', 38740),\n",
       " ('of', 28140),\n",
       " ('in', 21421),\n",
       " ('and', 20336),\n",
       " ('a', 17732),\n",
       " ('is', 15613),\n",
       " ('to', 12132),\n",
       " ('-RRB-', 10007),\n",
       " ('-LRB-', 9997),\n",
       " ('The', 8377),\n",
       " ('was', 8071),\n",
       " (\"''\", 7984),\n",
       " ('as', 5829),\n",
       " ('for', 5151),\n",
       " ('by', 5077),\n",
       " ('on', 4854),\n",
       " ('with', 3983),\n",
       " ('from', 3736),\n",
       " ('that', 3611),\n",
       " (\"'s\", 3536),\n",
       " ('are', 3402),\n",
       " ('an', 3328),\n",
       " ('or', 3224),\n",
       " ('at', 2829),\n",
       " ('It', 2641),\n",
       " ('In', 2356),\n",
       " ('his', 2350),\n",
       " ('which', 2274),\n",
       " ('it', 2240),\n",
       " (':', 2035),\n",
       " ('also', 1987),\n",
       " ('has', 1850),\n",
       " ('be', 1848),\n",
       " ('he', 1848),\n",
       " (';', 1798),\n",
       " ('first', 1757),\n",
       " ('born', 1738),\n",
       " ('were', 1636),\n",
       " ('He', 1623),\n",
       " ('one', 1525),\n",
       " ('who', 1382),\n",
       " ('France', 1365),\n",
       " ('its', 1321),\n",
       " ('United', 1318),\n",
       " ('not', 1306),\n",
       " ('known', 1301),\n",
       " ('have', 1283),\n",
       " ('city', 1220),\n",
       " ('commune', 1197),\n",
       " ('other', 1168),\n",
       " ('but', 1146),\n",
       " ('their', 1143),\n",
       " ('used', 1124),\n",
       " ('-', 1111),\n",
       " ('had', 1094),\n",
       " ('A', 1092),\n",
       " ('two', 1027),\n",
       " ('States', 1018),\n",
       " ('most', 1016),\n",
       " ('--', 1010),\n",
       " ('American', 997),\n",
       " ('department', 997),\n",
       " ('can', 968),\n",
       " ('called', 967),\n",
       " ('after', 939),\n",
       " ('been', 932),\n",
       " ('this', 896),\n",
       " ('region', 888),\n",
       " ('football', 879),\n",
       " ('they', 875),\n",
       " ('into', 852),\n",
       " ('her', 844),\n",
       " ('name', 837),\n",
       " ('about', 831),\n",
       " ('people', 818),\n",
       " ('many', 816),\n",
       " ('made', 804),\n",
       " ('when', 793),\n",
       " ('part', 781),\n",
       " (\"'\", 778),\n",
       " ('such', 767),\n",
       " ('more', 759),\n",
       " ('This', 750),\n",
       " ('between', 746),\n",
       " ('New', 727),\n",
       " ('became', 719),\n",
       " ('player', 703),\n",
       " ('than', 689),\n",
       " ('only', 683),\n",
       " ('time', 681),\n",
       " ('found', 667),\n",
       " ('all', 665),\n",
       " ('years', 651),\n",
       " ('some', 646),\n",
       " ('World', 615),\n",
       " ('&', 611),\n",
       " ('1', 605),\n",
       " ('English', 605),\n",
       " ('over', 602),\n",
       " ('former', 596),\n",
       " ('state', 565),\n",
       " ('during', 561),\n",
       " ('team', 555),\n",
       " ('where', 555),\n",
       " ('They', 551),\n",
       " ('area', 551),\n",
       " ('de', 539),\n",
       " ('British', 536),\n",
       " ('located', 532),\n",
       " ('January', 528),\n",
       " ('â', 527),\n",
       " ('June', 526),\n",
       " ('released', 519),\n",
       " ('September', 518),\n",
       " ('three', 518),\n",
       " ('2', 516),\n",
       " ('town', 512),\n",
       " ('new', 511),\n",
       " ('album', 509),\n",
       " ('April', 503),\n",
       " ('October', 500),\n",
       " ('December', 500),\n",
       " ('second', 499),\n",
       " ('March', 498),\n",
       " ('may', 497),\n",
       " ('later', 492),\n",
       " ('County', 490),\n",
       " ('there', 487),\n",
       " ('series', 486),\n",
       " ('she', 485),\n",
       " ('year', 483),\n",
       " ('July', 482),\n",
       " ('up', 478),\n",
       " ('German', 478),\n",
       " ('On', 474),\n",
       " ('band', 474),\n",
       " ('number', 470),\n",
       " ('August', 469),\n",
       " ('November', 469),\n",
       " ('named', 467),\n",
       " ('May', 467),\n",
       " ('England', 451),\n",
       " ('music', 450),\n",
       " ('University', 448),\n",
       " ('including', 448),\n",
       " ('until', 448),\n",
       " ('often', 447),\n",
       " ('use', 446),\n",
       " ('under', 443),\n",
       " ('then', 440),\n",
       " ('2007', 438),\n",
       " ('National', 438),\n",
       " ('North', 436),\n",
       " ('being', 435),\n",
       " ('family', 431),\n",
       " ('well', 431),\n",
       " ('South', 430),\n",
       " ('both', 428),\n",
       " ('system', 425),\n",
       " ('film', 425),\n",
       " ('February', 424),\n",
       " ('would', 424),\n",
       " ('She', 423),\n",
       " ('world', 423),\n",
       " ('him', 420),\n",
       " ('War', 416),\n",
       " ('game', 415),\n",
       " ('season', 414),\n",
       " ('through', 414),\n",
       " ('London', 405),\n",
       " ('French', 403),\n",
       " ('very', 394),\n",
       " ('`', 393),\n",
       " ('population', 392),\n",
       " ('ndash', 392),\n",
       " ('north', 391),\n",
       " ('them', 391),\n",
       " ('century', 390),\n",
       " ('played', 390),\n",
       " ('before', 385),\n",
       " ('same', 383),\n",
       " ('since', 383),\n",
       " ('small', 379),\n",
       " ('John', 373),\n",
       " ('largest', 371),\n",
       " ('2008', 371),\n",
       " ('After', 371),\n",
       " ('four', 371),\n",
       " ('while', 370),\n",
       " ('district', 370),\n",
       " ('There', 369),\n",
       " ('Germany', 369),\n",
       " ('will', 368),\n",
       " ('group', 367),\n",
       " ('usually', 366),\n",
       " ('Japanese', 365),\n",
       " ('out', 365),\n",
       " ('3', 362),\n",
       " ('won', 362),\n",
       " ('capital', 361),\n",
       " ('II', 361),\n",
       " ('League', 360),\n",
       " ('City', 359),\n",
       " ('several', 357),\n",
       " ('large', 355),\n",
       " ('because', 354),\n",
       " ('2006', 354),\n",
       " ('died', 348),\n",
       " ('King', 347),\n",
       " ('national', 344),\n",
       " ('km', 344),\n",
       " ('northern', 343),\n",
       " ('any', 342),\n",
       " ('now', 341),\n",
       " ('early', 340),\n",
       " ('around', 338),\n",
       " ('plays', 331),\n",
       " ('work', 331),\n",
       " ('based', 331),\n",
       " ('2005', 330),\n",
       " ('each', 330),\n",
       " ('York', 329),\n",
       " ('main', 328),\n",
       " ('10', 327),\n",
       " ('began', 324),\n",
       " ('language', 323),\n",
       " ('%', 321),\n",
       " ('different', 321),\n",
       " ('I', 319),\n",
       " ('near', 319),\n",
       " ('no', 316),\n",
       " ('best', 315),\n",
       " ('government', 315),\n",
       " ('4', 312),\n",
       " ('television', 307),\n",
       " ('million', 307),\n",
       " ('so', 303),\n",
       " ('form', 303),\n",
       " ('common', 302),\n",
       " ('major', 301),\n",
       " ('south', 300),\n",
       " ('5', 298),\n",
       " ('As', 298),\n",
       " ('Australia', 297),\n",
       " ('15', 296),\n",
       " ('country', 296),\n",
       " ('River', 296),\n",
       " ('like', 295),\n",
       " ('Kingdom', 293),\n",
       " ('end', 290),\n",
       " ('species', 289),\n",
       " ('sometimes', 288),\n",
       " ('America', 286),\n",
       " ('U.S.', 284),\n",
       " ('2009', 284),\n",
       " ('currently', 283),\n",
       " ('One', 283),\n",
       " ('these', 282),\n",
       " ('written', 281),\n",
       " ('place', 281),\n",
       " ('His', 281),\n",
       " ('important', 281),\n",
       " ('term', 281),\n",
       " ('include', 279),\n",
       " ('professional', 277),\n",
       " ('municipality', 276),\n",
       " ('West', 275),\n",
       " ('province', 275),\n",
       " ('make', 274),\n",
       " ('Ã', 273),\n",
       " ('water', 273),\n",
       " ('6', 265),\n",
       " ('created', 265),\n",
       " ('20', 264),\n",
       " ('President', 262),\n",
       " ('single', 262),\n",
       " ('famous', 260),\n",
       " ('Japan', 260),\n",
       " ('do', 260),\n",
       " ('death', 260),\n",
       " ('long', 259),\n",
       " ('popular', 258),\n",
       " ('For', 257),\n",
       " ('last', 256),\n",
       " ('16', 256),\n",
       " ('2004', 256),\n",
       " ('along', 256),\n",
       " ('against', 255),\n",
       " ('8', 254),\n",
       " ('12', 253),\n",
       " ('Europe', 252),\n",
       " ('song', 252),\n",
       " ('home', 250),\n",
       " ('2000', 250),\n",
       " ('rock', 250),\n",
       " ('history', 250),\n",
       " ('published', 250),\n",
       " ('life', 249),\n",
       " ('book', 249),\n",
       " ('William', 248),\n",
       " ('started', 246),\n",
       " ('show', 245),\n",
       " ('However', 245),\n",
       " ('East', 245),\n",
       " ('30', 245),\n",
       " ('At', 244),\n",
       " ('25', 244),\n",
       " ('held', 243),\n",
       " ('formed', 241),\n",
       " ('18', 241),\n",
       " ('founded', 240),\n",
       " ('given', 240),\n",
       " ('24', 239),\n",
       " ('When', 239),\n",
       " ('members', 238),\n",
       " ('James', 238),\n",
       " ('Roman', 237),\n",
       " ('built', 236),\n",
       " ('countries', 236),\n",
       " ('Italian', 235),\n",
       " ('son', 235),\n",
       " ('11', 235),\n",
       " ('southern', 235),\n",
       " ('title', 233),\n",
       " ('7', 233),\n",
       " ('club', 232),\n",
       " ('county', 230),\n",
       " ('third', 230),\n",
       " ('example', 229),\n",
       " ('following', 228),\n",
       " ('said', 228),\n",
       " ('set', 228),\n",
       " ('back', 227),\n",
       " ('13', 227),\n",
       " ('District', 226),\n",
       " ('considered', 224),\n",
       " ('did', 224),\n",
       " ('play', 223),\n",
       " ('Pas-de-Calais', 223),\n",
       " ('old', 223),\n",
       " ('member', 222),\n",
       " ('Republic', 221),\n",
       " ('school', 221),\n",
       " ('own', 221),\n",
       " ('produced', 220),\n",
       " ('George', 218),\n",
       " ('Switzerland', 217),\n",
       " ('17', 217),\n",
       " ('within', 216),\n",
       " ('Some', 216),\n",
       " ('19', 216),\n",
       " ('another', 216),\n",
       " ('Pakistan', 216),\n",
       " ('Union', 215),\n",
       " ('west', 215),\n",
       " ('public', 214),\n",
       " ('high', 214),\n",
       " ('day', 214),\n",
       " ('games', 214),\n",
       " ('still', 213),\n",
       " ('2003', 212),\n",
       " ('means', 211),\n",
       " ('see', 211),\n",
       " ('video', 211),\n",
       " ('European', 211),\n",
       " ('2002', 210),\n",
       " ('period', 210),\n",
       " ('father', 210),\n",
       " ('Its', 208),\n",
       " ('Spanish', 208),\n",
       " ('California', 208),\n",
       " ('Greek', 207),\n",
       " ('much', 207),\n",
       " ('14', 207),\n",
       " ('took', 206),\n",
       " ('become', 206),\n",
       " ('word', 206),\n",
       " ('Canada', 205),\n",
       " ('Great', 205),\n",
       " ('age', 204),\n",
       " ('An', 204),\n",
       " ('=', 203),\n",
       " ('21', 203),\n",
       " ('modern', 203),\n",
       " ('left', 203),\n",
       " ('State', 202),\n",
       " ('if', 202),\n",
       " ('These', 202),\n",
       " ('using', 201),\n",
       " ('Nord-Pas-de-Calais', 201),\n",
       " ('could', 200),\n",
       " ('developed', 200),\n",
       " ('St.', 200),\n",
       " ('During', 200),\n",
       " ('island', 199),\n",
       " ('India', 198),\n",
       " ('east', 198),\n",
       " ('official', 197),\n",
       " ('2001', 196),\n",
       " ('even', 196),\n",
       " ('9', 196),\n",
       " ('five', 196),\n",
       " ('body', 196),\n",
       " ('live', 195),\n",
       " ('moved', 195),\n",
       " ('Charles', 194),\n",
       " ('original', 194),\n",
       " ('line', 193),\n",
       " ('just', 193),\n",
       " ('movie', 192),\n",
       " ('power', 190),\n",
       " ('Ð', 189),\n",
       " ('served', 189),\n",
       " ('footballer', 188),\n",
       " ('way', 187),\n",
       " ('current', 187),\n",
       " ('International', 187),\n",
       " ('Aisne', 186),\n",
       " ('came', 185),\n",
       " ('times', 185),\n",
       " ('Party', 184),\n",
       " ('land', 184),\n",
       " ('late', 184),\n",
       " ('type', 183),\n",
       " ('commonly', 183),\n",
       " ('China', 182),\n",
       " ('referred', 182),\n",
       " ('does', 182),\n",
       " ('School', 181),\n",
       " ('Empire', 181),\n",
       " ('parts', 181),\n",
       " ('tropical', 181),\n",
       " ('central', 181),\n",
       " ('order', 181),\n",
       " ('Royal', 180),\n",
       " ('22', 180),\n",
       " ('similar', 180),\n",
       " ('29', 180),\n",
       " ('children', 179),\n",
       " ('version', 179),\n",
       " ('what', 179),\n",
       " ('canton', 178),\n",
       " ('2010', 178),\n",
       " ('person', 177),\n",
       " ('originally', 174),\n",
       " ('23', 174),\n",
       " ('although', 173),\n",
       " ('those', 173),\n",
       " ('College', 173),\n",
       " ('company', 172),\n",
       " ('Championship', 171),\n",
       " ('few', 171),\n",
       " ('center', 171),\n",
       " ('Calvados', 171),\n",
       " ('1999', 170),\n",
       " ('days', 169),\n",
       " ('due', 169),\n",
       " ('Most', 169),\n",
       " ('26', 168),\n",
       " ('received', 168),\n",
       " ('record', 168),\n",
       " ('character', 168),\n",
       " ('local', 167),\n",
       " ('Italy', 167),\n",
       " ('Ireland', 166),\n",
       " ('Australian', 166),\n",
       " ('event', 165),\n",
       " ('hurricane', 165),\n",
       " ('recorded', 165),\n",
       " ('actor', 164),\n",
       " ('next', 163),\n",
       " ('without', 163),\n",
       " ('works', 163),\n",
       " ('Basse-Normandie', 162),\n",
       " ('short', 162),\n",
       " ('Western', 161),\n",
       " ('Park', 161),\n",
       " ('meaning', 161),\n",
       " ('career', 161),\n",
       " ('international', 161),\n",
       " ('Ø', 161),\n",
       " ('playing', 160),\n",
       " ('1\\\\/4', 160),\n",
       " ('six', 159),\n",
       " ('making', 159),\n",
       " ('included', 158),\n",
       " ('wrote', 158),\n",
       " ('Henry', 158),\n",
       " ('village', 158),\n",
       " ('includes', 158),\n",
       " ('down', 158),\n",
       " ('singer', 158),\n",
       " ('Football', 157),\n",
       " ('together', 157),\n",
       " ('Island', 156),\n",
       " ('states', 156),\n",
       " ('28', 156),\n",
       " ('History', 156),\n",
       " ('metal', 156),\n",
       " ('role', 155),\n",
       " ('head', 155),\n",
       " ('Other', 155),\n",
       " ('27', 155),\n",
       " ('Africa', 155),\n",
       " ('went', 155),\n",
       " ('political', 154),\n",
       " ('general', 154),\n",
       " ('#', 154),\n",
       " ('station', 154),\n",
       " ('Grand', 153),\n",
       " ('Robert', 153),\n",
       " ('again', 153),\n",
       " ('Church', 153),\n",
       " ('storm', 152),\n",
       " ('off', 152),\n",
       " ('discovered', 152),\n",
       " ('m', 151),\n",
       " ('Earth', 151),\n",
       " ('First', 149),\n",
       " ('III', 149),\n",
       " ('1997', 149),\n",
       " ('top', 149),\n",
       " ('David', 148),\n",
       " ('color', 148),\n",
       " ('point', 148),\n",
       " ('areas', 148),\n",
       " ('WWE', 148),\n",
       " ('Asia', 147),\n",
       " ('Scotland', 147),\n",
       " ('law', 147),\n",
       " ('systems', 147),\n",
       " ('right', 147),\n",
       " ('Christian', 147),\n",
       " ('Paul', 146),\n",
       " ('among', 146),\n",
       " ('mother', 146),\n",
       " ('total', 145),\n",
       " ('eastern', 145),\n",
       " ('war', 145),\n",
       " ('Council', 145),\n",
       " ('side', 145),\n",
       " ('d.', 144),\n",
       " ('married', 144),\n",
       " ('wife', 144),\n",
       " ('Many', 143),\n",
       " ('1998', 143),\n",
       " ('list', 143),\n",
       " ('Central', 143),\n",
       " ('seen', 142),\n",
       " ('La', 142),\n",
       " ('1996', 142),\n",
       " ('taken', 141),\n",
       " ('Â', 141),\n",
       " ('House', 141),\n",
       " ('living', 140),\n",
       " ('though', 140),\n",
       " ('uses', 139),\n",
       " ('river', 139),\n",
       " ('light', 139),\n",
       " ('computer', 138),\n",
       " ('great', 138),\n",
       " ('worked', 138),\n",
       " ('Brazilian', 138),\n",
       " ('San', 138),\n",
       " ('$', 138),\n",
       " ('generally', 137),\n",
       " ('various', 137),\n",
       " ('Atlantic', 137),\n",
       " ('All', 137),\n",
       " ('songs', 137),\n",
       " ('Cup', 137),\n",
       " ('match', 136),\n",
       " ('however', 136),\n",
       " ('white', 136),\n",
       " ('According', 136),\n",
       " ('led', 136),\n",
       " ('BC', 135),\n",
       " ('miles', 135),\n",
       " ('Latin', 135),\n",
       " ('size', 135),\n",
       " ('Florida', 135),\n",
       " ('especially', 135),\n",
       " ('Edward', 135),\n",
       " ('daughter', 134),\n",
       " ('release', 134),\n",
       " ('Northern', 133),\n",
       " ('Indian', 133),\n",
       " ('Paris', 133),\n",
       " ('Red', 132),\n",
       " ('established', 132),\n",
       " ('actress', 132),\n",
       " ('changed', 132),\n",
       " ('every', 132),\n",
       " ('composer', 132),\n",
       " ('Minister', 131),\n",
       " ('better', 131),\n",
       " ('across', 131),\n",
       " ('types', 131),\n",
       " ('Queen', 130),\n",
       " ('Catholic', 130),\n",
       " ('full', 129),\n",
       " ('less', 129),\n",
       " ('languages', 129),\n",
       " ('almost', 129),\n",
       " ('teams', 129),\n",
       " ('Award', 128),\n",
       " ('Music', 128),\n",
       " ('free', 128),\n",
       " ('Britain', 128),\n",
       " ('groups', 128),\n",
       " ('Berlin', 128),\n",
       " ('western', 128),\n",
       " ('natural', 128),\n",
       " ('Emperor', 127),\n",
       " ('force', 127),\n",
       " ('least', 127),\n",
       " ('either', 127),\n",
       " ('joined', 126),\n",
       " ('Sea', 126),\n",
       " ('support', 126),\n",
       " ('energy', 126),\n",
       " ('C', 125),\n",
       " ('science', 125),\n",
       " ('human', 125),\n",
       " ('If', 125),\n",
       " ('1992', 125),\n",
       " ('final', 124),\n",
       " ('With', 124),\n",
       " ('others', 124),\n",
       " ('sea', 124),\n",
       " ('food', 124),\n",
       " ('Russian', 124),\n",
       " ('service', 124),\n",
       " ('story', 123),\n",
       " ('young', 123),\n",
       " ('mostly', 123),\n",
       " ('study', 123),\n",
       " ('Canadian', 123),\n",
       " ('books', 122),\n",
       " ('simply', 122),\n",
       " ('surface', 122),\n",
       " ('Although', 122),\n",
       " ('brother', 122),\n",
       " ('While', 122),\n",
       " ('Soviet', 122),\n",
       " ('1994', 122),\n",
       " ('players', 122),\n",
       " ('author', 121),\n",
       " ('UK', 121),\n",
       " ('information', 121),\n",
       " ('1975', 121),\n",
       " ('General', 121),\n",
       " ('lived', 121),\n",
       " ('Chinese', 121),\n",
       " ('characters', 121),\n",
       " ('politician', 121),\n",
       " ('leader', 121),\n",
       " ('Louis', 120),\n",
       " ('elected', 120),\n",
       " ('level', 120),\n",
       " ('mainly', 120),\n",
       " ('1980', 120),\n",
       " ('above', 120),\n",
       " ('military', 120),\n",
       " ('how', 120),\n",
       " ('1974', 120),\n",
       " ('present', 120),\n",
       " ('building', 120),\n",
       " ('art', 120),\n",
       " ('gave', 120),\n",
       " ('Î', 120),\n",
       " ('Since', 119),\n",
       " ('la', 119),\n",
       " ('From', 119),\n",
       " ('ice', 119),\n",
       " ('black', 119),\n",
       " ('writer', 119),\n",
       " ('1981', 119),\n",
       " ('Day', 118),\n",
       " ('officially', 118),\n",
       " ('chemical', 118),\n",
       " ('software', 118),\n",
       " ('contains', 118),\n",
       " ('highest', 118),\n",
       " ('opened', 118),\n",
       " ('women', 117),\n",
       " ('development', 117),\n",
       " ('Disney', 117),\n",
       " ('must', 117),\n",
       " ('1995', 117),\n",
       " ('should', 117),\n",
       " ('sold', 116),\n",
       " ('By', 116),\n",
       " ('having', 116),\n",
       " ('Michael', 116),\n",
       " ('1979', 115),\n",
       " ('Washington', 115),\n",
       " ('US', 114),\n",
       " ('run', 114),\n",
       " ('good', 114),\n",
       " ('office', 114),\n",
       " ('features', 114),\n",
       " ('director', 114),\n",
       " ('program', 114),\n",
       " ('Scottish', 114),\n",
       " ('result', 114),\n",
       " ('range', 114),\n",
       " ('position', 114),\n",
       " ('working', 114),\n",
       " ('Spain', 113),\n",
       " ('appeared', 113),\n",
       " ('throughout', 113),\n",
       " ('Islands', 113),\n",
       " ('lead', 113),\n",
       " ('page', 113),\n",
       " ('thought', 113),\n",
       " ('refer', 113),\n",
       " ('stage', 113),\n",
       " ('you', 113),\n",
       " ('ancient', 113),\n",
       " ('Ù', 113),\n",
       " ('seat', 112),\n",
       " ('take', 112),\n",
       " ('1982', 112),\n",
       " ('itself', 112),\n",
       " ('novel', 112),\n",
       " ('s', 112),\n",
       " ('1993', 112),\n",
       " ('party', 112),\n",
       " ('comes', 112),\n",
       " ('31', 112),\n",
       " ('1973', 112),\n",
       " ('lost', 112),\n",
       " ('shows', 111),\n",
       " ('plant', 111),\n",
       " ('coast', 111),\n",
       " ('100', 111),\n",
       " ('Games', 110),\n",
       " ('Hall', 110),\n",
       " ('Super', 110),\n",
       " ('Iowa', 110),\n",
       " ('1991', 110),\n",
       " ('Province', 110),\n",
       " ('killed', 110),\n",
       " ('Her', 109),\n",
       " ('Parliament', 109),\n",
       " ('Gironde', 109),\n",
       " ('half', 109),\n",
       " ('performed', 109),\n",
       " ('1986', 109),\n",
       " ('site', 109),\n",
       " ('n', 108),\n",
       " ('go', 108),\n",
       " ('man', 108),\n",
       " ('hit', 108),\n",
       " ('consists', 108),\n",
       " ('Eastern', 108),\n",
       " ('open', 108),\n",
       " ('northwestern', 107),\n",
       " ('designed', 107),\n",
       " ('independent', 107),\n",
       " ('caused', 107),\n",
       " ('himself', 107),\n",
       " ('1984', 107),\n",
       " ('Mary', 107),\n",
       " ('Nintendo', 107),\n",
       " ('1990', 107),\n",
       " ('outside', 107),\n",
       " ('Chicago', 107),\n",
       " ('today', 107),\n",
       " ('Lake', 107),\n",
       " ('Richard', 107),\n",
       " ('signed', 107),\n",
       " ('leading', 107),\n",
       " ('once', 107),\n",
       " ('continued', 107),\n",
       " ('community', 106),\n",
       " ('church', 106),\n",
       " ('Hockey', 106),\n",
       " ('men', 106),\n",
       " ('unit', 106),\n",
       " ('makes', 106),\n",
       " ('per', 106),\n",
       " ('Virginia', 105),\n",
       " ('Wrestling', 105),\n",
       " ('Records', 105),\n",
       " ('network', 105),\n",
       " ('plants', 105),\n",
       " ('league', 105),\n",
       " ('events', 104),\n",
       " ('studio', 104),\n",
       " ('numbers', 104),\n",
       " ('smaller', 104),\n",
       " ('Black', 104),\n",
       " ('Sir', 104),\n",
       " ('help', 104),\n",
       " ('related', 104),\n",
       " ('traditional', 104),\n",
       " ('available', 104),\n",
       " ('further', 104),\n",
       " ('animals', 103),\n",
       " ('university', 103),\n",
       " ('units', 103),\n",
       " ('cities', 103),\n",
       " ('instead', 102),\n",
       " ('Wales', 102),\n",
       " ('movement', 102),\n",
       " ('Mexico', 102),\n",
       " ('border', 102),\n",
       " ('Windows', 102),\n",
       " ('never', 102),\n",
       " ('Battle', 102),\n",
       " ('debut', 102),\n",
       " ('Prime', 102),\n",
       " ('Academy', 102),\n",
       " ('divided', 101),\n",
       " ('islands', 101),\n",
       " ('strong', 101),\n",
       " ('Bay', 101),\n",
       " ('names', 101),\n",
       " ('!', 101),\n",
       " ('musical', 101),\n",
       " ('oldest', 101),\n",
       " ('Ocean', 101),\n",
       " ('1978', 101),\n",
       " ('average', 101),\n",
       " ('data', 100),\n",
       " ('Old', 100),\n",
       " ('Association', 100),\n",
       " ('calendar', 100),\n",
       " ('Their', 100),\n",
       " ('northwest', 100),\n",
       " ('forces', 100),\n",
       " ('fourth', 100),\n",
       " ('control', 100),\n",
       " ('Dutch', 100),\n",
       " ('refers', 100),\n",
       " ('1987', 100),\n",
       " ('theory', 99),\n",
       " ('Los', 99),\n",
       " ('von', 99),\n",
       " ('Aquitaine', 99),\n",
       " ('described', 99),\n",
       " ('far', 99),\n",
       " ('native', 99),\n",
       " ('returned', 98),\n",
       " ('Center', 98),\n",
       " ('Prince', 98),\n",
       " ('production', 98),\n",
       " ('king', 98),\n",
       " ('close', 98),\n",
       " ('Mario', 98),\n",
       " ('length', 98),\n",
       " ('sports', 98),\n",
       " ('1967', 98),\n",
       " ('You', 97),\n",
       " ('Oklahoma', 97),\n",
       " ('awarded', 97),\n",
       " ('away', 97),\n",
       " ('reached', 97),\n",
       " ('key', 96),\n",
       " ('starting', 96),\n",
       " ('approximately', 96),\n",
       " ('Thomas', 96),\n",
       " ('announced', 95),\n",
       " ('minor', 95),\n",
       " ('months', 95),\n",
       " (\"n't\", 95),\n",
       " ('widely', 95),\n",
       " ('services', 95),\n",
       " ('female', 95),\n",
       " ('special', 95),\n",
       " ('design', 95),\n",
       " ('Peter', 95),\n",
       " ('1968', 95),\n",
       " ('White', 94),\n",
       " ('High', 94),\n",
       " ('Picardie', 94),\n",
       " ('child', 94),\n",
       " ('films', 94),\n",
       " ('beginning', 94),\n",
       " ('house', 94),\n",
       " ('1983', 94),\n",
       " ('Army', 94),\n",
       " ('20th', 94),\n",
       " ('seven', 94),\n",
       " ('stadium', 94),\n",
       " ('Hurricane', 93),\n",
       " ('get', 93),\n",
       " ('date', 93),\n",
       " ('Because', 93),\n",
       " ('radio', 93),\n",
       " ('1969', 93),\n",
       " ('1989', 93),\n",
       " ('Saint', 93),\n",
       " ('Wikipedia', 93),\n",
       " ('class', 93),\n",
       " ('1977', 93),\n",
       " ('car', 93),\n",
       " ('Year', 93),\n",
       " ('race', 93),\n",
       " ('episode', 92),\n",
       " ('process', 92),\n",
       " ('operating', 92),\n",
       " ('larger', 92),\n",
       " ('genus', 92),\n",
       " ('added', 91),\n",
       " ('standard', 91),\n",
       " ('Division', 91),\n",
       " ('success', 91),\n",
       " ('Angeles', 91),\n",
       " ('southwest', 91),\n",
       " ('Netherlands', 91),\n",
       " ('come', 91),\n",
       " ('material', 91),\n",
       " ('Belgian', 91),\n",
       " ('successful', 91),\n",
       " ('Jewish', 91),\n",
       " ('shown', 91),\n",
       " ('NHL', 91),\n",
       " ('rather', 91),\n",
       " ('low', 91),\n",
       " ('retired', 90),\n",
       " ('forms', 90),\n",
       " ('whose', 90),\n",
       " ('upon', 90),\n",
       " ('writing', 90),\n",
       " ('J.', 90),\n",
       " ('certain', 90),\n",
       " ('Best', 89),\n",
       " ('fiction', 89),\n",
       " ('See', 89),\n",
       " ('Pope', 89),\n",
       " ('little', 89),\n",
       " ('God', 89),\n",
       " ('1971', 89),\n",
       " ('students', 89),\n",
       " ('case', 89),\n",
       " ('Texas', 88),\n",
       " ('Duke', 88),\n",
       " ('Harry', 88),\n",
       " ('red', 88),\n",
       " ('albums', 88),\n",
       " ('higher', 88),\n",
       " ('1972', 88),\n",
       " ('Formula', 88),\n",
       " ('Victoria', 87),\n",
       " ('1970', 87),\n",
       " ('change', 87),\n",
       " ('hockey', 87),\n",
       " ('1985', 87),\n",
       " ('cells', 87),\n",
       " ('style', 87),\n",
       " ('brought', 87),\n",
       " ('terms', 87),\n",
       " ('social', 86),\n",
       " ('1988', 86),\n",
       " ('1945', 86),\n",
       " ('To', 86),\n",
       " ('cell', 86),\n",
       " ('cause', 86),\n",
       " ('districts', 86),\n",
       " ('1976', 86),\n",
       " ('possible', 86),\n",
       " ('culture', 86),\n",
       " ('money', 86),\n",
       " ('sent', 86),\n",
       " ('+', 86),\n",
       " ('1947', 86),\n",
       " ('Pacific', 86),\n",
       " ('Entertainment', 85),\n",
       " ('disease', 85),\n",
       " ('Department', 85),\n",
       " ('longer', 85),\n",
       " ('Joseph', 85),\n",
       " ('Society', 85),\n",
       " ('introduced', 85),\n",
       " ('BBC', 85),\n",
       " ('article', 85),\n",
       " ('earlier', 85),\n",
       " ('middle', 85),\n",
       " ('organization', 85),\n",
       " ('brand', 85),\n",
       " ('championship', 85),\n",
       " ('create', 85),\n",
       " ('Carolina', 85),\n",
       " ('Company', 85),\n",
       " ('hard', 85),\n",
       " ('territory', 84),\n",
       " ('1966', 84),\n",
       " ('Early', 84),\n",
       " ('words', 84),\n",
       " ('addition', 84),\n",
       " ('able', 84),\n",
       " ('opera', 84),\n",
       " ('composed', 84),\n",
       " ('manager', 84),\n",
       " ('Big', 84),\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_vocab(test_full['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67405c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = len(label2idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "943aa6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(72090, 768)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add unknown words into pretrained model\n",
    "new_tokens = check_vocab(test_full['original_text'], local_vocab,\"unknown_list\")\n",
    "\n",
    "# check if the tokens are already in the vocabulary\n",
    "new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "\n",
    "# add the tokens to the tokenizer vocabulary\n",
    "print(len(new_tokens))\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "\n",
    "# add new, random embeddings for the new tokens\n",
    "#model.train()\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e70ba379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(72090, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a6c50fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_create_trie',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'basic_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'deprecation_warnings',\n",
       " 'do_basic_tokenize',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'ids_to_tokens',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size',\n",
       " 'wordpiece_tokenizer']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef15f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72090"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae4dff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e0b0a",
   "metadata": {},
   "source": [
    "it looks like the added vocabulary is stored in other place instead of standard vocabulary since the total vocabulary is not changed even there are new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c6eb895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['henry', 'wage', '##r', 'halle', '##ck']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Henry Wager Halleck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe848e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Henry Wager Halleck\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dda16ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89e87815a254c74b40ae0acf57f52eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors performance: (0.0, 0.0, 0.0, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    5545.0\n",
      "           1       0.00      0.00      0.00    4045.0\n",
      "\n",
      "    accuracy                           0.00    9590.0\n",
      "   macro avg       0.00      0.00      0.00    9590.0\n",
      "weighted avg       0.00      0.00      0.00    9590.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 1, ..., 1, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the error data should not be predicted correctly.\n",
    "texts=list(error_data[\"original_text\"])\n",
    "labels=list(error_data[\"label\"])\n",
    "data_evaluation(texts,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206fdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb35efd3",
   "metadata": {},
   "source": [
    "#### Spelling error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a15ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct spelling error to check whether accuracy could be improved.\n",
    "#from textblob import TextBlob\n",
    "#error_data['correct_text_0'] = error_data['original_text'].apply(lambda x :TextBlob(x).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e160051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi welcome to spelling', 'this is just an example but consider a very big corpus']\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "import re\n",
    "\n",
    "spell=Speller(lang=\"en\")\n",
    "WORD = re.compile(r'\\w+')\n",
    "def reTokenize(doc):\n",
    "    tokens = WORD.findall(doc)\n",
    "    return tokens\n",
    "\n",
    "text = [\"Hi, welcmoe to speling.\",\"This is jsut an exapmle, but cosnider a veri big coprus.\"]\n",
    "def spell_correct(text):\n",
    "    sptext = []\n",
    "    for doc in text:\n",
    "        sptext.append(' '.join([spell(w).lower() for w in reTokenize(doc)]))      \n",
    "    return sptext    \n",
    "\n",
    "print(spell_correct(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b79b8b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9602/9602 [1:03:13<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "correct_text=[]\n",
    "for i in trange(len(error_data)):\n",
    "    correct_text.append(TextBlob(error_data['original_text'].iloc[i]).correct())\n",
    "error_data['correct_text_0'] = correct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f17be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9602/9602 [08:55<00:00, 17.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "correct_text=[]\n",
    "for i in trange(len(error_data)):\n",
    "    correct_text.append(' '.join([spell(w).lower() for w in reTokenize(error_data['original_text'].iloc[i])]))\n",
    "error_data['correct_text_1'] = correct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0819fc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d092686569604625814d05bb8286a179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors performance: (0.13903353468027493, 0.13903353468027493, 0.13903353468027493, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.19      0.21      6058\n",
      "           1       0.04      0.06      0.05      3544\n",
      "\n",
      "    accuracy                           0.14      9602\n",
      "   macro avg       0.15      0.12      0.13      9602\n",
      "weighted avg       0.17      0.14      0.15      9602\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 1, 0, ..., 1, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=list(error_data[\"correct_text_0\"])\n",
    "labels=list(error_data[\"label\"])\n",
    "data_evaluation(texts,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ead2051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>correct_text_0</th>\n",
       "      <th>correct_text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Second-wave feminism , also sometimes called w...</td>\n",
       "      <td>1</td>\n",
       "      <td>(S, e, c, o, n, d, -, w, a, v, e,  , f, e, m, ...</td>\n",
       "      <td>second wave feminism also sometimes called wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In an episode where Raw returned to the Manhat...</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, n,  , a, n,  , e, p, i, s, o, d, e,  , w, ...</td>\n",
       "      <td>in an episode where raw returned to the manhat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Military bases Virginia Beach is home to sever...</td>\n",
       "      <td>0</td>\n",
       "      <td>(M, i, l, i, t, a, r, y,  , b, a, s, e, s,  , ...</td>\n",
       "      <td>military bases virginia beach is home to sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In mathematics , hyperbolic geometry is a non-...</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, n,  , m, a, t, h, e, m, a, t, i, c, s,  , ...</td>\n",
       "      <td>in mathematics hyperbolic geometry is a non eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Events The largest horse show in Norway , the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(E, v, e, n, t, s,  , T, h, e,  , l, a, r, g, ...</td>\n",
       "      <td>events the largest horse show in norway the ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>9597</td>\n",
       "      <td>is an electronic music group created by German...</td>\n",
       "      <td>0</td>\n",
       "      <td>(i, s,  , a, n,  , e, l, e, c, t, r, o, n, i, ...</td>\n",
       "      <td>is an electronic music group created by german...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9598</th>\n",
       "      <td>9598</td>\n",
       "      <td>Polly Pocket is a line of small plastic dolls ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(F, o, l, l, y,  , P, o, c, k, e, t,  , i, s, ...</td>\n",
       "      <td>poll pocket is a line of small plastic dolls a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599</th>\n",
       "      <td>9599</td>\n",
       "      <td>In a few special cases , an unreleased album m...</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, n,  , a,  , f, e, w,  , s, p, e, c, i, a, ...</td>\n",
       "      <td>in a few special cases an unreleased album may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>9600</td>\n",
       "      <td>Aside from this central belief , its ideology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(S, i, d, e,  , f, r, o, m,  , t, h, i, s,  , ...</td>\n",
       "      <td>aside from this central belief its ideology is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>9601</td>\n",
       "      <td>The Fraser Institute , however , criticized th...</td>\n",
       "      <td>0</td>\n",
       "      <td>(T, h, e,  , F, r, a, s, e, r,  , I, n, s, t, ...</td>\n",
       "      <td>the fraser institute however criticized this s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9602 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      original_text  label  \\\n",
       "0              0  Second-wave feminism , also sometimes called w...      1   \n",
       "1              1  In an episode where Raw returned to the Manhat...      0   \n",
       "2              2  Military bases Virginia Beach is home to sever...      0   \n",
       "3              3  In mathematics , hyperbolic geometry is a non-...      0   \n",
       "4              4  Events The largest horse show in Norway , the ...      1   \n",
       "...          ...                                                ...    ...   \n",
       "9597        9597  is an electronic music group created by German...      0   \n",
       "9598        9598  Polly Pocket is a line of small plastic dolls ...      0   \n",
       "9599        9599  In a few special cases , an unreleased album m...      0   \n",
       "9600        9600  Aside from this central belief , its ideology ...      0   \n",
       "9601        9601  The Fraser Institute , however , criticized th...      0   \n",
       "\n",
       "                                         correct_text_0  \\\n",
       "0     (S, e, c, o, n, d, -, w, a, v, e,  , f, e, m, ...   \n",
       "1     (I, n,  , a, n,  , e, p, i, s, o, d, e,  , w, ...   \n",
       "2     (M, i, l, i, t, a, r, y,  , b, a, s, e, s,  , ...   \n",
       "3     (I, n,  , m, a, t, h, e, m, a, t, i, c, s,  , ...   \n",
       "4     (E, v, e, n, t, s,  , T, h, e,  , l, a, r, g, ...   \n",
       "...                                                 ...   \n",
       "9597  (i, s,  , a, n,  , e, l, e, c, t, r, o, n, i, ...   \n",
       "9598  (F, o, l, l, y,  , P, o, c, k, e, t,  , i, s, ...   \n",
       "9599  (I, n,  , a,  , f, e, w,  , s, p, e, c, i, a, ...   \n",
       "9600  (S, i, d, e,  , f, r, o, m,  , t, h, i, s,  , ...   \n",
       "9601  (T, h, e,  , F, r, a, s, e, r,  , I, n, s, t, ...   \n",
       "\n",
       "                                         correct_text_1  \n",
       "0     second wave feminism also sometimes called wom...  \n",
       "1     in an episode where raw returned to the manhat...  \n",
       "2     military bases virginia beach is home to sever...  \n",
       "3     in mathematics hyperbolic geometry is a non eu...  \n",
       "4     events the largest horse show in norway the ar...  \n",
       "...                                                 ...  \n",
       "9597  is an electronic music group created by german...  \n",
       "9598  poll pocket is a line of small plastic dolls a...  \n",
       "9599  in a few special cases an unreleased album may...  \n",
       "9600  aside from this central belief its ideology is...  \n",
       "9601  the fraser institute however criticized this s...  \n",
       "\n",
       "[9602 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b64ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2c63cd951241da88c5cc7c919b415c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors performance: (0.24421995417621328, 0.24421995417621328, 0.24421995417621328, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.34      0.36      6058\n",
      "           1       0.07      0.08      0.08      3544\n",
      "\n",
      "    accuracy                           0.24      9602\n",
      "   macro avg       0.23      0.21      0.22      9602\n",
      "weighted avg       0.27      0.24      0.26      9602\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 1, 1, ..., 1, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=list(error_data[\"correct_text_1\"])\n",
    "labels=list(error_data[\"label\"])\n",
    "data_evaluation(texts,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943046d",
   "metadata": {},
   "source": [
    "##### The above result suggests spelling check could improve 13% accuracy, however, the performance of spelling check is not encouraging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def19571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_data.to_csv(\"./tmp/error_data_20221006.csv\",index=False) # export error data to csv for manual check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097188d",
   "metadata": {},
   "source": [
    "#### Clean text to see whether that would improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=list(error_data[\"clean_text\"])\n",
    "labels=list(error_data[\"label\"])\n",
    "data_evaluation(texts,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import to_lower, remove_punctuation,remove_special_character,normalize_unicode,check_spelling,remove_stopword,lemmatize_word\n",
    "# Preprocess text using custom preprocess functions in the pipeline \n",
    "STOPWORDS=['-RRB-','-LRB-'] # remove customized stopwords\n",
    "preprocess_functions = [to_lower, remove_punctuation,remove_special_character,normalize_unicode,remove_stopword,lemmatize_word]\n",
    "error_data['preprocess_text'] = error_data['correct_text_1'].apply(lambda x:' '.join(remove_stopword(preprocess_text(x,preprocess_functions),\n",
    "                                                                                          stop_words=STOPWORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2502970",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5add739",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=list(error_data[\"preprocess_text\"])\n",
    "labels=list(error_data[\"label\"])\n",
    "data_evaluation(texts,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b243c6f",
   "metadata": {},
   "source": [
    "#### The accuracy of error list was improved to 34%, and it looks like the preprocessing text do help to improve accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59cbfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=\"./01_data/WikiLarge_Train.csv\"\n",
    "train_data=pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8048d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 416768/416768 [5:00:37<00:00, 23.11it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_text=[]\n",
    "for i in trange(len(train_data)):\n",
    "    correct_text.append(' '.join([spell(w).lower() for w in reTokenize(train_data['original_text'].iloc[i])]))\n",
    "train_data['correct_text_1'] = correct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08884ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"./tmp/train_data_correction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692f39f",
   "metadata": {},
   "source": [
    "##### Given the above improvement, and try to apply this to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8325e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data for submission\n",
    "sub_data_path=\"./01_data/WikiLarge_Test.csv\"\n",
    "sub_data=pd.read_csv(sub_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3457cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 119092/119092 [1:26:52<00:00, 22.85it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_text=[]\n",
    "for i in trange(len(sub_data)):\n",
    "    correct_text.append(' '.join([spell(w).lower() for w in reTokenize(sub_data['original_text'].iloc[i])]))\n",
    "sub_data['correct_text_1'] = correct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee62584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_data['original_text'] = sub_data['correct_text_1'].apply(lambda x:' '.join(remove_stopword(preprocess_text(x,preprocess_functions),\n",
    "#                                                                                          stop_words=STOPWORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "146cb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.to_csv(\"./tmp/sub_data_correction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e17289bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Test size: 119092\n"
     ]
    }
   ],
   "source": [
    "sub_texts=list(sub_data[\"correct_text_1\"])\n",
    "sub_labels=[1 for i in range(len(sub_texts))]\n",
    "\n",
    "print(\"Submission Test size:\", len(sub_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82aa4915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00777ab89f7c4fc8a70b50cd7bb9fa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/7444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors performance: (1.0, 1.0, 1.0, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    119092\n",
      "\n",
      "    accuracy                           1.00    119092\n",
      "   macro avg       1.00      1.00      1.00    119092\n",
      "weighted avg       1.00      1.00      1.00    119092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, sub_predicted=data_evaluation(sub_texts,sub_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00ce1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the submission file\n",
    "df_sub=pd.DataFrame(columns=[\"id\",\"label\"])\n",
    "df_sub['label']=sub_predicted\n",
    "df_sub['id']=[i for i in range(len(sub_predicted))]\n",
    "df_sub.to_csv(\"./tmp/submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e40c0",
   "metadata": {},
   "source": [
    "### Embedding-based clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b04163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other utils\n",
    "from tqdm import tqdm  # Progress bar\n",
    "# Transformers\n",
    "from transformers import pipeline\n",
    "import ipywidgets as widgets\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onlinemodel='distiluse-base-multilingual-cased-v2'\n",
    "onlinemodel=\"all-mpnet-base-v2\"\n",
    "embedder = SentenceTransformer(onlinemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries= list(error_data['original_text'])\n",
    "query_embeddings=embedder.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Elbow criterion - Determine optimal numbers of clusters by elbow rule.\n",
    "def elbow_plot(data, maxK=15, seed_centroids=None):\n",
    "    \"\"\"\n",
    "        parameters:\n",
    "        - data: pandas DataFrame (data to be fitted)\n",
    "        - maxK (default = 10): integer (maximum number of clusters with which to run k-means)\n",
    "        - seed_centroids (default = None ): float (initial value of centroids for k-means)\n",
    "    \"\"\"\n",
    "    sse = []\n",
    "    K= range(1, maxK)\n",
    "    for k in K:\n",
    "        if seed_centroids is not None:\n",
    "            seeds = seed_centroids.head(k)\n",
    "            kmeans = KMeans(n_clusters=k, max_iter=500, n_init=100, random_state=0, init=np.reshape(seeds, (k,1))).fit(data)\n",
    "            #data[\"clusters\"] = kmeans.labels_\n",
    "        else:\n",
    "            kmeans = KMeans(n_clusters=k, max_iter=300, n_init=100, random_state=0).fit(data)\n",
    "            #data[\"clusters\"] = kmeans.labels_\n",
    "        print(\"k: \", k,\"sse: \",kmeans.inertia_)\n",
    "        # Inertia: Sum of distances of samples to their closest cluster center\n",
    "        sse.append(kmeans.inertia_)\n",
    "    plt.figure()\n",
    "    plt.plot(K,sse,'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Elbow for full training data\n",
    "elbow_plot(query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd266e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clustering dataframe \n",
    "def df_clustering(queries, embeddings, labels=None, clusters=2):\n",
    "    \"\"\"\n",
    "        parameters:\n",
    "        - queries: list of queries\n",
    "        - embeddings: list of embeddings corresponding to queries\n",
    "        - clusters: no. of clusters for kmeans\n",
    "    \"\"\"\n",
    "    num_clusters = clusters\n",
    "    clf = KMeans(n_clusters=num_clusters, \n",
    "                max_iter=100, \n",
    "                init='k-means++', \n",
    "                n_init=1)\n",
    "    clf.fit_predict(embeddings)\n",
    "    cluster_assignment = clf.labels_\n",
    "\n",
    "    cdf=pd.DataFrame(columns=[\"cluster_id\",\"sentence_id\",\"sentence\",'label'])\n",
    "\n",
    "    cdf['cluster_id']=clf.labels_\n",
    "    cdf['sentence_id']=[i for i in range(len(clf.labels_))]\n",
    "    cdf['sentence']=queries\n",
    "    cdf['label']=labels\n",
    "                                            \n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "clf = KMeans(n_clusters=num_clusters, \n",
    "            max_iter=100, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "clf.fit_predict(query_embeddings)\n",
    "cluster_assignment = clf.labels_\n",
    "\n",
    "cdf=pd.DataFrame(columns=[\"cluster_id\",\"sentence_id\",\"sentence\"])\n",
    "\n",
    "for i in range(len(cluster_assignment)):\n",
    "    new_row=pd.DataFrame({\"cluster_id\":[cluster_assignment[i]],\n",
    "                                \"sentence_id\":[i],\n",
    "                                \"sentence\":[queries[i]]\n",
    "                           })\n",
    "    cdf=pd.concat([cdf,new_row],axis=0,ignore_index=True)\n",
    "\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254067b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA to reduce the dimension to project the result to 2-d scatter plot\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(query_embeddings)\n",
    "\n",
    "df_pca = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "df_pca['sentence']=queries\n",
    "# Combine PCA results with K-means results to see clustering\n",
    "df_k=df_pca.merge(cdf,right_on=['sentence'],left_on=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94adcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ls_clusters=pd.unique(df_k[\"cluster_id\"])\n",
    "ls_colors=['tab:blue', 'tab:orange', 'tab:green','tab:purple']\n",
    "#ls_colors=df_k[\"cluster_id\"].astype('category').cat.codes\n",
    "for id in range(len(ls_clusters)):\n",
    "    ax.scatter(df_k[df_k['cluster_id']==ls_clusters[id]]['principal component 1'],\n",
    "               df_k[df_k['cluster_id']==ls_clusters[id]]['principal component 2'], c=ls_colors[id], label=ls_clusters[id],\n",
    "               alpha=0.9, edgecolors='none')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b29b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
