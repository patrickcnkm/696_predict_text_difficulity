{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc61cc2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "####  Data analysis & visualiztion on the skills of data scientists from the job description of 2 hiring websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c575fc0-0ddf-4a8b-a0d1-ca9958194261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following if it's required to install the following packages\n",
    "#!pip install altair pyLDAvis tqdm dtale transformers sentence_transformers spacy scattertext\n",
    "#!pip install Flask==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b656938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:09:30.917706Z",
     "start_time": "2022-05-24T13:09:30.847089Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/image_utils.py:239: DeprecationWarning:\n",
      "\n",
      "BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/image_utils.py:396: DeprecationWarning:\n",
      "\n",
      "NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/h5py/__init__.py:46: DeprecationWarning:\n",
      "\n",
      "`np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing\n",
    "import os,re\n",
    "\n",
    "# Disable warning of 3 types\n",
    "import warnings\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import altair as alt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "# Other utils\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "#EDA tools.\n",
    "import dtale\n",
    "\n",
    "# nlp text cleaning\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline\n",
    "import ipywidgets as widgets\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Visualizing text\n",
    "import spacy\n",
    "import scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1d0c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:57:41.656630Z",
     "start_time": "2022-05-23T12:57:41.573082Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "train_data_path=\"./01_data/WikiLarge_Train.csv\"\n",
    "train_data=pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a17c8a",
   "metadata": {},
   "source": [
    "## Embedding-based clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181c71fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:58:17.840572Z",
     "start_time": "2022-05-23T12:57:42.943915Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#onlinemodel='bert-large-nli-mean-tokens'\n",
    "onlinemodel='distiluse-base-multilingual-cased-v2'\n",
    "embedder = SentenceTransformer(onlinemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff76874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T15:46:05.862384Z",
     "start_time": "2022-05-24T15:41:22.390856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n",
      " 50%|█████     | 104326/208384 [1:02:33<1:03:05, 27.49it/s]"
     ]
    }
   ],
   "source": [
    "#queries = list(jobs['job_desc'][0:30])\n",
    "queries_0 = list(train_data[train_data['label']==0]['original_text'])\n",
    "query_embeddings_0=[]\n",
    "for item in tqdm(queries_0):\n",
    "    query_embeddings_0.append(embedder.encode([item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060f227-5683-491e-b903-27736d247c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afcef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T15:56:08.917844Z",
     "start_time": "2022-05-24T15:56:08.883043Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_sim(i,queries,embeddings,threshold=0.9):\n",
    "    np_em=np.array(embeddings)\n",
    "    sim=cosine_similarity([embeddings[i]],np_em[0:])\n",
    "    \n",
    "    sim[np.where(sim>=1)]=0\n",
    "    #print(sim)\n",
    "    x=np.argmax(sim)\n",
    "    \n",
    "    if sim[0][x]>=threshold:\n",
    "        #print(x,sim[0][x],queries[i],queries[x])\n",
    "        return queries[x]\n",
    "    else:\n",
    "        return queries[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be78b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:19:44.235711Z",
     "start_time": "2022-05-23T13:06:56.213250Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Elbow criterion - Determine optimal numbers of clusters by elbow rule.\n",
    "def elbow_plot(data, maxK=15, seed_centroids=None):\n",
    "    \"\"\"\n",
    "        parameters:\n",
    "        - data: pandas DataFrame (data to be fitted)\n",
    "        - maxK (default = 10): integer (maximum number of clusters with which to run k-means)\n",
    "        - seed_centroids (default = None ): float (initial value of centroids for k-means)\n",
    "    \"\"\"\n",
    "    sse = []\n",
    "    K= range(1, maxK)\n",
    "    for k in K:\n",
    "        if seed_centroids is not None:\n",
    "            seeds = seed_centroids.head(k)\n",
    "            kmeans = KMeans(n_clusters=k, max_iter=500, n_init=100, random_state=0, init=np.reshape(seeds, (k,1))).fit(data)\n",
    "            #data[\"clusters\"] = kmeans.labels_\n",
    "        else:\n",
    "            kmeans = KMeans(n_clusters=k, max_iter=300, n_init=100, random_state=0).fit(data)\n",
    "            #data[\"clusters\"] = kmeans.labels_\n",
    "        print(\"k: \", k,\"sse: \",kmeans.inertia_)\n",
    "        # Inertia: Sum of distances of samples to their closest cluster center\n",
    "        sse.append(kmeans.inertia_)\n",
    "    plt.figure()\n",
    "    plt.plot(K,sse,'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    return kmeans.labels_\n",
    "\n",
    "# Run Elbow\n",
    "elbow_plot(query_embeddings_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90999961",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:20:11.332906Z",
     "start_time": "2022-05-23T13:19:44.276200Z"
    }
   },
   "outputs": [],
   "source": [
    "# As clustering algorithm KMeams is a perfect fit.\n",
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters=num_clusters, \n",
    "            max_iter=100, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "clf.fit_predict(query_embeddings_0)\n",
    "cluster_assignment = clf.labels_\n",
    "\n",
    "cdf=pd.DataFrame(columns=[\"cluster_id\",\"sentence_id\",\"sentence\"])\n",
    "\n",
    "for i in range(len(cluster_assignment)):\n",
    "    new_row=pd.Series(data={\"cluster_id\":cluster_assignment[i],\n",
    "                                \"sentence_id\":i,\n",
    "                                \"sentence\":queries_0[i]\n",
    "                           }\n",
    "                            )\n",
    "    cdf=cdf.append(new_row,ignore_index=True)\n",
    "\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02017529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:20:13.179075Z",
     "start_time": "2022-05-23T13:20:12.022360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using PCA to reduce the dimension to project the result to 2-d scatter plot\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(query_embeddings_0)\n",
    "\n",
    "df_pca = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "df_pca['sentence']=queries_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d07cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:20:13.356670Z",
     "start_time": "2022-05-23T13:20:13.219147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform EDA to check clustering result\n",
    "d2 = dtale.show(df_pca)\n",
    "d2.open_browser()\n",
    "# Using PCA, it could not clearly identify different group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba935cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:20:13.454814Z",
     "start_time": "2022-05-23T13:20:13.422731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine PCA results with K-means results to see clustering\n",
    "df_k=df_pca.merge(cdf,right_on=['sentence'],left_on=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e41775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:20:18.091874Z",
     "start_time": "2022-05-23T13:20:14.911942Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(df_main['description_cln'])\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(df_main['description_cln'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e17826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:20:57.159038Z",
     "start_time": "2022-05-23T13:20:18.111039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using LDA to cluster skills\n",
    "lda_tf = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b5509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:21:11.866248Z",
     "start_time": "2022-05-23T13:20:57.181672Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe353397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:21:11.906457Z",
     "start_time": "2022-05-23T13:21:11.891603Z"
    }
   },
   "outputs": [],
   "source": [
    "# The above result present topic modelling may not suggest meaningful clustering.\n",
    "# However, it may suggest I can combine all skills into sentence, then using topic modelling to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86145ff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:21:11.976378Z",
     "start_time": "2022-05-23T13:21:11.922124Z"
    }
   },
   "outputs": [],
   "source": [
    "df_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a91e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:21:12.799068Z",
     "start_time": "2022-05-23T13:21:11.987753Z"
    }
   },
   "outputs": [],
   "source": [
    "    height=600\n",
    "    width=800\n",
    "    # Create scatter plot to display death and confirmed cases by countries\n",
    "    scat=alt.Chart(df_k).mark_circle(size=100).encode(\n",
    "        y=alt.Y(\"principal component 1\", axis=alt.Axis(format='f', title='PC 1')),\n",
    "        x=alt.X(\"principal component 2\",axis=alt.Axis(format='f', title='PC 2')),\n",
    "        #color=alt.condition(select_country,alt.value(\"red\"),alt.value(\"#66B2FF\")),\n",
    "        color='cluster_id',\n",
    "        tooltip=[\"skills\"]\n",
    "    ).properties(\n",
    "        height=height, width=width,\n",
    "        title = alt.TitleParams(text = 'Clusters of skills',\n",
    "                                anchor='middle',\n",
    "                                font = 'Ubuntu Mono', \n",
    "                                fontSize = 16, \n",
    "                                color = '#3E454F', \n",
    "                                )\n",
    "    )\n",
    "    scat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcfb39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:21:12.837855Z",
     "start_time": "2022-05-23T13:21:12.829145Z"
    }
   },
   "outputs": [],
   "source": [
    "# The clustering of embedding based also has no clue on the skills clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f53d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:20.613388Z",
     "start_time": "2022-05-23T13:25:19.955552Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c63c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:20.649287Z",
     "start_time": "2022-05-23T13:25:20.621633Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=20):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(X, features, row_id, top_n=25):\n",
    "    row = np.squeeze(X[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(X, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    if grp_ids:\n",
    "        D = X[grp_ids].toarray()\n",
    "    else:\n",
    "        D = X.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7619344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:21.292771Z",
     "start_time": "2022-05-23T13:25:20.680088Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2436e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:22.003442Z",
     "start_time": "2022-05-23T13:25:21.317478Z"
    }
   },
   "outputs": [],
   "source": [
    "# At this point we are going to tokenize the bodies and convert them\n",
    "# into a document-term matrix.\n",
    "\n",
    "# Some note on min_df and max_df\n",
    "# max_df=0.5 means \"ignore all terms that appear in more then 50% of the documents\"\n",
    "# min_df=2 means \"ignore all terms that appear in less then 2 documents\"\n",
    "stopwords = ENGLISH_STOP_WORDS.union(['data','scientist'])\n",
    "vect = TfidfVectorizer(analyzer='word', stop_words=stopwords, max_df=0.5, min_df=2,use_idf=True,max_features=200000, \n",
    "                       tokenizer=tokenize_only,ngram_range=(1,3))\n",
    "\n",
    "X = vect.fit_transform(df_tk['titles'].values)\n",
    "print(X.shape)\n",
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6de309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:22.053074Z",
     "start_time": "2022-05-23T13:25:22.015362Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_feats_per_cluster(X, y, features, min_tfidf=0.1, top_n=25):\n",
    "    dfs = []\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label) \n",
    "        feats_df = top_mean_feats(X, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    fig = plt.figure(figsize=(15, 9+len(dfs)*6), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(len(dfs),3, i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"cluster = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.score, align='center', color='#7530FF')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.features)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2f604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:23.599535Z",
     "start_time": "2022-05-23T13:25:22.074185Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use this to print the top terms per cluster with matplotlib.\n",
    "plot_tfidf_classfeats_h(top_feats_per_cluster(X, t_clf.labels_, features, 0.1, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfd53f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:23.627224Z",
     "start_time": "2022-05-23T13:25:23.609612Z"
    }
   },
   "outputs": [],
   "source": [
    "# The above is more than clear, the tiltes are consisted of 3 types: junior, senior, and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685606cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:23.832476Z",
     "start_time": "2022-05-23T13:25:23.816644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Decide to divide the job postings into 3 types:junior, senior, and others\n",
    "def ds_level(title=''):\n",
    "    jr = [\"junior\", \"jr\", \"jr.\",\"intern\",\"internship\",\"young\",\"student\",\"analyst\",\"associate\"]\n",
    "    sr = [\"sr.\",\"sr\",\"senior\",\"lead\",\"leading\",\"principal\",\"president\"]\n",
    "\n",
    "    if any(x in title.lower() for x in jr):\n",
    "        return \"junior\"\n",
    "\n",
    "    if any(x in title.lower() for x in sr):\n",
    "        return \"senior\"\n",
    "    \n",
    "    return \"others\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1983cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:23.914966Z",
     "start_time": "2022-05-23T13:25:23.856750Z"
    }
   },
   "outputs": [],
   "source": [
    "df_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b844c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:23.971502Z",
     "start_time": "2022-05-23T13:25:23.929759Z"
    }
   },
   "outputs": [],
   "source": [
    "df_main['type']=df_main['title'].apply(lambda x: ds_level(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb461c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:24.011179Z",
     "start_time": "2022-05-23T13:25:23.994207Z"
    }
   },
   "outputs": [],
   "source": [
    "df_main['id']=df_main['id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c2d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:24.562871Z",
     "start_time": "2022-05-23T13:25:24.275675Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full=df_skills.merge(df_main,how='left',left_on=['id'],right_on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3360a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:27.448358Z",
     "start_time": "2022-05-23T13:25:24.640947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform EDA to check main table\n",
    "d4 = dtale.show(df_full)\n",
    "d4.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7b0d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:25:34.442656Z",
     "start_time": "2022-05-23T13:25:27.463650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using scattertext to visualize the skills by types\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a83f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:30:55.799182Z",
     "start_time": "2022-05-23T13:25:34.465572Z"
    }
   },
   "outputs": [],
   "source": [
    "    corpus = (scattertext.CorpusFromPandas(df_full,\n",
    "                                           category_col='type', \n",
    "                                           text_col='skill',\n",
    "                                           nlp=nlp)\n",
    "              .build()\n",
    "              .remove_terms(nlp.Defaults.stop_words, ignore_absences=True)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482052f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:30:55.921417Z",
     "start_time": "2022-05-23T13:30:55.827781Z"
    }
   },
   "outputs": [],
   "source": [
    "df = corpus.get_term_freq_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e946a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:30:56.054409Z",
     "start_time": "2022-05-23T13:30:55.952931Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bebb0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:31:01.715627Z",
     "start_time": "2022-05-23T13:30:56.099214Z"
    }
   },
   "outputs": [],
   "source": [
    "html = scattertext.produce_scattertext_explorer(\n",
    "                   corpus,\n",
    "                   category='senior',\n",
    "                   category_name='senior',\n",
    "                   not_category_name=['junior'],\n",
    "                   width_in_pixels=1000,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa06b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:31:01.813609Z",
     "start_time": "2022-05-23T13:31:01.730750Z"
    }
   },
   "outputs": [],
   "source": [
    "#open(\"ds_skills.html\", 'wb').write(html.encode('utf-8'))\n",
    "#with open(\"ds_skills.html\", 'w') as outf: outf.write(html)\n",
    "from IPython.display import IFrame    \n",
    "display(IFrame(\"ds_skills.html\", width=900, height=650))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575849a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:09:41.903919Z",
     "start_time": "2022-05-24T13:09:41.066465Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# data clean for data jobs\n",
    "data_file= './01_Data/Output/datajobs.csv'\n",
    "df_data=pd.read_csv(data_file)\n",
    "# Drop the duplicated job postings \n",
    "df_data.drop_duplicates(subset=['employer','description','title','location'],inplace=True)\n",
    "# Drop the job posting with same id even the above would be a little different.\n",
    "df_data.drop_duplicates(subset=['id'],inplace=True)\n",
    "# drop na\n",
    "df_data.dropna(subset=['description'],inplace=True)\n",
    "# Change string to datetime\n",
    "df_data['posting_date']=df_data['posting_date'].apply(lambda x: parser.parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd15a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:18:23.857799Z",
     "start_time": "2022-05-24T13:18:23.759023Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove data scientists job from data jobs\n",
    "print(len(df_data))\n",
    "ds_list=df_main['id'].unique()\n",
    "df_data['id']=df_data['id']=df_data['id'].apply(lambda x: None if x in (ds_list) else x)\n",
    "df_data.dropna(subset=['id'],inplace=True)\n",
    "print(len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ee1ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:24:15.699582Z",
     "start_time": "2022-05-24T13:18:45.953601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert titles to embedding\n",
    "titles = list(df_data['title'].unique())\n",
    "titles_embeddings = embedder.encode(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b9851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:26:09.928133Z",
     "start_time": "2022-05-24T13:25:10.232041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run Elbow to decide K for k-means\n",
    "elbow_plot(titles_embeddings,maxK=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63442d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:26:27.556268Z",
     "start_time": "2022-05-24T13:26:20.419343Z"
    }
   },
   "outputs": [],
   "source": [
    "# As clustering algorithm KMeams is a perfect fit.\n",
    "num_clusters = 3\n",
    "t_clf = KMeans(n_clusters=num_clusters, \n",
    "            max_iter=100, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "t_clf.fit_predict(titles_embeddings)\n",
    "t_cluster_assignment = t_clf.labels_\n",
    "\n",
    "t_cdf=pd.DataFrame(columns=[\"cluster_id\",\"sentence_id\",\"sentence\"])\n",
    "\n",
    "for i in range(len(t_cluster_assignment)):\n",
    "    new_row=pd.Series(data={\"cluster_id\":t_cluster_assignment[i],\n",
    "                                \"sentence_id\":i,\n",
    "                                \"sentence\":titles[i]\n",
    "                           }\n",
    "                            )\n",
    "    t_cdf=t_cdf.append(new_row,ignore_index=True)\n",
    "\n",
    "t_cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a70d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:26:52.553120Z",
     "start_time": "2022-05-24T13:26:52.450489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using PCA to reduce the dimension to project the result to 2-d scatter plot\n",
    "\n",
    "t_pca = PCA(n_components=2)\n",
    "t_principalComponents = t_pca.fit_transform(titles_embeddings)\n",
    "\n",
    "\n",
    "df_tpca = pd.DataFrame(data = t_principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "df_tpca['titles']=titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1640a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:28:30.482734Z",
     "start_time": "2022-05-24T13:28:30.457168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine PCA results with K-means results to see clustering\n",
    "df_tk=df_tpca.merge(t_cdf,right_on=['sentence'],left_on=['titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542e9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac064d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:28:35.160857Z",
     "start_time": "2022-05-24T13:28:34.473282Z"
    }
   },
   "outputs": [],
   "source": [
    "# At this point we are going to tokenize the bodies and convert them\n",
    "# into a document-term matrix.\n",
    "\n",
    "# Some note on min_df and max_df\n",
    "# max_df=0.5 means \"ignore all terms that appear in more then 50% of the documents\"\n",
    "# min_df=2 means \"ignore all terms that appear in less then 2 documents\"\n",
    "stopwords = ENGLISH_STOP_WORDS.union(['data','scientist'])\n",
    "vect = TfidfVectorizer(analyzer='word', stop_words=stopwords, max_df=0.5, min_df=2,use_idf=True,max_features=200000, \n",
    "                       tokenizer=tokenize_only,ngram_range=(1,3))\n",
    "\n",
    "X = vect.fit_transform(df_tk['titles'].values)\n",
    "print(X.shape)\n",
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214998dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:29:02.313799Z",
     "start_time": "2022-05-24T13:29:02.028561Z"
    }
   },
   "outputs": [],
   "source": [
    "    height=600\n",
    "    width=800\n",
    "    # Create scatter plot to display death and confirmed cases by countries\n",
    "    tscat=alt.Chart(df_tk).mark_circle(size=100).encode(\n",
    "        y=alt.Y(\"principal component 1\", axis=alt.Axis(format='f', title='PC 1')),\n",
    "        x=alt.X(\"principal component 2\",axis=alt.Axis(format='f', title='PC 2')),\n",
    "        #color=alt.condition(select_country,alt.value(\"red\"),alt.value(\"#66B2FF\")),\n",
    "        color=alt.Color('cluster_id', scale=alt.Scale(scheme='accent')),\n",
    "        tooltip=[\"titles\"]\n",
    "    ).properties(\n",
    "        height=height, width=width,\n",
    "        title = alt.TitleParams(text = 'Clusters of titles',\n",
    "                                anchor='middle',\n",
    "                                font = 'Ubuntu Mono', \n",
    "                                fontSize = 16, \n",
    "                                color = '#3E454F', \n",
    "                                )\n",
    "    )\n",
    "    tscat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deabca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:30:28.904037Z",
     "start_time": "2022-05-24T13:30:27.394383Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use this to print the top terms per cluster with matplotlib.\n",
    "plot_tfidf_classfeats_h(top_feats_per_cluster(X, t_clf.labels_, features, 0.1, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b944433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:38:15.913949Z",
     "start_time": "2022-05-24T13:38:15.889417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select data scientist jobs\n",
    "print(len(df_data))\n",
    "df_de=df_data[df_data['title'].str.contains(r'^(?=.*data)(?=.*engineer)',case=False)]\n",
    "print(len(df_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e0cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
