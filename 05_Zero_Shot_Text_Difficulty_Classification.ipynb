{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d8f5fb",
   "metadata": {},
   "source": [
    "## Zero-Shot Text Difficulty Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c875e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Zero-shot and few-shot NLP models are used to handle NLP given the limited dataset. This note is trying to explore those algorithms to improve the accuracy of text classification in terms of text difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0908d",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769c3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model can be loaded with the zero-shot-classification pipeline like so:\n",
    "\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9277c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['difficult', 'easy'],\n",
       " 'scores': [0.8136769533157349, 0.18632300198078156]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this pipeline to classify sequences into any of the class names you specify.\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['difficult','easy']\n",
    "\n",
    "classifier(sequence_to_classify, candidate_labels)\n",
    "#{'labels': ['travel', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n",
    "# 'sequence': 'one day I will see the world'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931593e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
       " 'scores': [0.994511067867279,\n",
       "  0.9383885264396667,\n",
       "  0.005706145893782377,\n",
       "  0.0018192846328020096]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label=True)\n",
    "#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9945111274719238,\n",
    "#  0.9383890628814697,\n",
    "#  0.0057061901316046715,\n",
    "#  0.0018193122232332826],\n",
    "# 'sequence': 'one day I will see the world'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda6031",
   "metadata": {},
   "source": [
    "#### Manual Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb096ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2335: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# run through model pre-trained on MNLI\u001b[39;00m\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(premise, hypothesis, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m                      truncation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly_first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m logits \u001b[38;5;241m=\u001b[39m nli_model(x\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# we throw away \"neutral\" (dim 1) and take the probability of\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# \"entailment\" (2) as the probability of the label being true \u001b[39;00m\n\u001b[0;32m     17\u001b[0m entail_contradiction_logits \u001b[38;5;241m=\u001b[39m logits[:,[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# pose sequence as a NLI premise and label as a hypothesis\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "premise = sequence_to_classify\n",
    "label=\"difficult\"\n",
    "hypothesis = f'This example is {label}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')\n",
    "logits = nli_model(x.to(device))[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f94c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
