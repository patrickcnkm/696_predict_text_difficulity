{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning RoBERTa by Huggingface for Fake News classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './01_data'\n",
    "output_path = './tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "#from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file with dataset. Perform basic transformations.\n",
    "train_data = pd.read_csv(f\"{data_path}/WikiLarge_Train.csv\")\n",
    "#df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#encode_label = {'FAKE' : 0, 'REAL' : 1}\n",
    "\n",
    "# Discard items with less than 5 words in text.\n",
    "#df = df[df.text.str.len() >= 5]\n",
    "\n",
    "#df['label'] = df['label'].map(encode_label)\n",
    "#df['titletext'] = df['title'] + \". \" + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'length')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram with the length. Truncate max length to 5000 tokens.\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "train_data['length'] = train_data['original_text'].apply(lambda x: len(x.split()))\n",
    "sns.distplot(train_data[train_data['length'] < 5000]['length'])\n",
    "plt.title('Frequence of documents of a given length', fontsize=14)\n",
    "plt.xlabel('length', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data, cropped to max length of the model.\n",
    "train_data['original_text'] = train_data['original_text'].apply(lambda x: \" \".join(x.split()[:512]))\n",
    "#df.to_csv(f\"{data_path}/prep_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and set device to GPU.\n",
    "torch.manual_seed(17)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "# Initialize tokenizer.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "size=round(len(train_data)*1)\n",
    "r_train=train_data.sample(n=size,random_state=1)\n",
    "texts=list(r_train[\"original_text\"])\n",
    "labels=list(r_train[\"label\"])\n",
    "    \n",
    "rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "target_names = list(set(labels))\n",
    "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_SEQ_LENGTH=80\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label2idx[label]\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "\n",
    "        \n",
    "    return input_items\n",
    "\n",
    "train_features = convert_examples_to_inputs(train_texts, train_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "dev_features = convert_examples_to_inputs(dev_texts, dev_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_inputs(test_texts, test_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    #dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    # dataloader tuning in https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "   \n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size,num_workers=2,pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "#BATCH_SIZE = 16\n",
    "# Tuning in https://mccormickml.com/2019/07/22/BERT-fine-tuning/#41-bertforsequenceclassification\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer hyperparameters.\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "\n",
    "# Define columns to read.\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "\n",
    "fields = {'titletext' : ('titletext', text_field), 'label' : ('label', label_field)}\n",
    "\n",
    "\n",
    "# Read preprocessed CSV into TabularDataset and split it into train, test and valid.\n",
    "train_data, valid_data, test_data = TabularDataset(path=f\"{data_path}/prep_news.csv\", \n",
    "                                                   format='CSV', \n",
    "                                                   fields=fields, \n",
    "                                                   skip_header=False).split(split_ratio=[0.70, 0.2, 0.1], \n",
    "                                                                            stratified=True, \n",
    "                                                                            strata_field='label')\n",
    "\n",
    "# Create train and validation iterators.\n",
    "train_iter, valid_iter = BucketIterator.splits((train_data, valid_data),\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               device=device,\n",
    "                                               shuffle=True,\n",
    "                                               sort_key=lambda x: len(x.titletext), \n",
    "                                               sort=True, \n",
    "                                               sort_within_batch=False)\n",
    "\n",
    "# Test iterator, no shuffling or sorting required.\n",
    "test_iter = Iterator(test_data, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving and loading model parameters and metrics.\n",
    "def save_checkpoint(path, model, valid_loss):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(path, model):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_metrics(path):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with extra layers on top of RoBERTa\n",
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1 = torch.nn.Linear(768, 64)\n",
    "        self.bn1 = torch.nn.LayerNorm(64)\n",
    "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _,x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #print(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.Tanh()(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(model, \n",
    "             optimizer, \n",
    "             train_iter, \n",
    "             valid_iter, \n",
    "             scheduler = None,\n",
    "             valid_period = len(train_dataloader),\n",
    "             num_epochs = 5):\n",
    "    \n",
    "    # Pretrain linear layers, do not train bert\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0   \n",
    "    global_step = 0  \n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "    #    for (source, target), _ in train_iter:\n",
    "         for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            #mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "            \n",
    "            y_pred = model(input_ids=input_ids,  \n",
    "                           attention_mask=input_mask)\n",
    "            #output = model(input_ids=source,\n",
    "            #              labels=target,\n",
    "            #              attention_mask=mask)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, label_ids)\n",
    "            \n",
    "        #for (source, target), _ in train_iter:\n",
    "            #mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "            \n",
    "            #y_pred = model(input_ids=source,  \n",
    "            #               attention_mask=mask)\n",
    "            \n",
    "            #loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "   \n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                        \n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], PT Loss: {:.4f}, Val Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "    \n",
    "    # Set bert parameters back to trainable\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print('Pre-training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          train_iter,\n",
    "          valid_iter,\n",
    "          scheduler = None,\n",
    "          num_epochs = 5,\n",
    "          valid_period = len(train_dataloader),\n",
    "          output_path = output_path):\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('Inf')\n",
    "    \n",
    "    global_step = 0\n",
    "    global_steps_list = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "    #    for (source, target), _ in train_iter:\n",
    "         for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            #mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "            y_pred = model(input_ids=input_ids,  \n",
    "                           attention_mask=input_mask)\n",
    "            #output = model(input_ids=source,\n",
    "            #              labels=target,\n",
    "            #              attention_mask=mask)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, label_ids)\n",
    "            #loss = output[0]\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "                        #output = model(input_ids=source,\n",
    "                        #               labels=target,\n",
    "                        #               attention_mask=mask)\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        #loss = output[0]\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                train_loss_list.append(train_loss)\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    save_checkpoint(output_path + '/model.pkl', model, best_valid_loss)\n",
    "                    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "                        \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "                model.train()\n",
    "    \n",
    "    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efa79814bba42eea7c033460d0508fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/21099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.25 GiB already allocated; 0 bytes free; 3.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10536\\3833246910.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mROBERTAClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.25 GiB already allocated; 0 bytes free; 3.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "NUM_EPOCHS = 6\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "model = ROBERTAClassifier(0.4)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Start pretraining ==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070bf4be527b4095abb7ec071872b60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/21099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not BaseModelOutputWithPoolingAndCrossAttentions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10536\\1241160110.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"======================= Start pretraining ==============================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m pretrain(model=model,\n\u001b[0m\u001b[0;32m     17\u001b[0m          \u001b[0mtrain_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m          \u001b[0mvalid_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdev_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10536\\2352396693.py\u001b[0m in \u001b[0;36mpretrain\u001b[1;34m(model, optimizer, train_iter, valid_iter, scheduler, valid_period, num_epochs)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m#mask = (source != PAD_INDEX).type(torch.uint8)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             y_pred = model(input_ids=input_ids,  \n\u001b[0m\u001b[0;32m     29\u001b[0m                            attention_mask=input_mask)\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m#output = model(input_ids=source,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10536\\598534813.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[1;34m\"but got {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not BaseModelOutputWithPoolingAndCrossAttentions"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "NUM_EPOCHS = 6\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "model = ROBERTAClassifier(0.4)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "print(\"======================= Start pretraining ==============================\")\n",
    "\n",
    "pretrain(model=model,\n",
    "         train_iter=train_dataloader,\n",
    "         valid_iter=dev_dataloader,\n",
    "         optimizer=optimizer,\n",
    "         scheduler=scheduler,\n",
    "         num_epochs=NUM_EPOCHS)\n",
    "\n",
    "NUM_EPOCHS = 12\n",
    "print(\"======================= Start training =================================\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*2, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "train(model=model, \n",
    "      train_iter=train_dataloader, \n",
    "      valid_iter=dev_dataloader, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler, \n",
    "      num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(output_path + '/metric.pkl')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (source, target), _ in test_loader:\n",
    "                mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                \n",
    "                output = model(source, attention_mask=mask)\n",
    "\n",
    "                y_pred.extend(torch.argmax(output, axis=-1).tolist())\n",
    "                y_true.extend(target.tolist())\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ROBERTAClassifier()\n",
    "model = model.to(device)\n",
    "\n",
    "load_checkpoint(output_path + '/model.pkl', model)\n",
    "\n",
    "evaluate(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "print(len(valid_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
