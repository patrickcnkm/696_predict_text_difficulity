{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d8f5fb",
   "metadata": {},
   "source": [
    "## Zero-Shot Text Difficulty Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c875e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Zero-shot and few-shot NLP models are used to handle NLP given the limited dataset. This note is trying to explore those algorithms to improve the accuracy of text classification in terms of text difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0908d",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769c3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model can be loaded with the zero-shot-classification pipeline like so:\n",
    "\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9277c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['difficult', 'easy'],\n",
       " 'scores': [0.8136769533157349, 0.18632300198078156]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this pipeline to classify sequences into any of the class names you specify.\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['difficult','easy']\n",
    "\n",
    "classifier(sequence_to_classify, candidate_labels)\n",
    "#{'labels': ['travel', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n",
    "# 'sequence': 'one day I will see the world'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931593e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
       " 'scores': [0.994511067867279,\n",
       "  0.9383885264396667,\n",
       "  0.005706145893782377,\n",
       "  0.0018192846328020096]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label=True)\n",
    "#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9945111274719238,\n",
    "#  0.9383890628814697,\n",
    "#  0.0057061901316046715,\n",
    "#  0.0018193122232332826],\n",
    "# 'sequence': 'one day I will see the world'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda6031",
   "metadata": {},
   "source": [
    "#### Manual Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb096ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose sequence as a NLI premise and label as a hypothesis\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "premise = sequence_to_classify\n",
    "label=1\n",
    "hypothesis = f'This example is {label}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')\n",
    "logits = nli_model(x.to(device))[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f94c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
