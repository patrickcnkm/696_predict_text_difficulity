{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3c1e9a",
   "metadata": {},
   "source": [
    "# Transformers Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64fc66",
   "metadata": {},
   "source": [
    "This note intends to pick up the best transformers from the popular types by:\n",
    "- Choosing the recommended parameters and comparing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Disable 3 types of warning\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=(FutureWarning))\n",
    "warnings.filterwarnings(\"ignore\",category=(RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b8492-bb75-4163-b8bb-bf66fede82af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d24553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Enable GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6348b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE=1\n",
    "#################### split data into train,dev,test##################\n",
    "def train_dev_test(dataset,random_state=RANDOM_STATE):\n",
    "    texts=list(dataset[\"original_text\"])\n",
    "    labels=list(dataset[\"label\"])\n",
    "    \n",
    "    target_names = list(set(labels))\n",
    "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "    print(label2idx)\n",
    "\n",
    "    rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=RANDOM_STATE)\n",
    "    train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(\"Train size:\", len(train_texts))\n",
    "    print(\"Dev size:\", len(dev_texts))\n",
    "    print(\"Test size:\", len(test_texts))\n",
    "    \n",
    "    #Create dataframe for coming issue analysis\n",
    "    df=pd.DataFrame()\n",
    "    df['original_text']=train_texts+test_texts\n",
    "    df['label']=train_labels+test_labels\n",
    "    df['id']=df.index\n",
    "    return df,(train_texts,dev_texts,test_texts),(train_labels,dev_labels,test_labels),(target_names,label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9041f",
   "metadata": {},
   "source": [
    "#### Prepare for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf81508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load train data\n",
    "source_train_data_path=\"./01_data/WikiLarge_Train.csv\"\n",
    "source_train_data=pd.read_csv(source_train_data_path)\n",
    "\n",
    "RANDOM_STATE=1\n",
    "PORTION=0.2\n",
    "size=round(len(source_train_data)*PORTION)\n",
    "train_data=source_train_data.sample(n=size,random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Plot histogram with the length. Truncate max length to 5000 tokens.\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "train_data['length'] = train_data['original_text'].apply(lambda x: len(x.split()))\n",
    "sns.distplot(train_data[train_data['length'] < 5000]['length'])\n",
    "plt.title('Frequence of sentences of a given length', fontsize=14)\n",
    "plt.xlabel('length', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT token length should not be more than 512.\n",
    "data_describe=train_data['original_text'].apply(lambda x: len(x.split())).describe()\n",
    "print(data_describe)\n",
    "MAX_SEQ_LENGTH=int(data_describe['max'])\n",
    "if MAX_SEQ_LENGTH>512:\n",
    "    MAX_SEQ_LENGTH=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9024051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train, dev, test data\n",
    "df_init,(train_texts,dev_texts,test_texts),(train_labels,dev_labels,test_labels),(target_names,label2idx)=train_dev_test(train_data,random_state=RANDOM_STATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#################### Both class and the following function are used to prepare for input items##################\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label2idx[label]\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "\n",
    "        \n",
    "    return input_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "#################### convert data for model input ##################\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    sample_ids=torch.tensor([j for j in range(len(features))], dtype=torch.long) #identify each record\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,sample_ids)\n",
    "\n",
    "    #dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    # dataloader tuning in https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "   \n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size,num_workers=2,pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead74ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids,_ = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #tmp_eval_loss, logits = model(input_ids, attention_mask=input_mask,\n",
    "            #                              token_type_ids=segment_ids, labels=label_ids)[:2]\n",
    "            tmp_eval_loss, logits = model(input_ids, attention_mask=input_mask,\n",
    "                                         labels=label_ids)[:2]  # for distilbert\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "        \n",
    "    return eval_loss, correct_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146075d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a7394-7c5e-4630-97d3-b5f14dd6eed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017079e-fa88-49f9-bc37-9f3a17cd71b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "import os\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "OUTPUT_DIR = \"./tmp/\"\n",
    "MODEL_FILE_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f73761-249a-4274-a047-24f22e0d09ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer,DistilBertForSequenceClassification,\\\n",
    "                         BertTokenizer, BertForSequenceClassification,\\\n",
    "                         RobertaTokenizer, RobertaForSequenceClassification,\\\n",
    "                         XLMRobertaTokenizer, XLMRobertaForSequenceClassification,\\\n",
    "                         AlbertTokenizer, AlbertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f3b5f-8334-422e-8871-45f3586f5b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transformers=pd.DataFrame(columns=[\"MODEL\",\"MODEL_NAME\",\"TOKENIZER\",\"CLASSIFIER\",\"OUTPUT_DIR\",\"MODEL_FILE_NAME\",\"PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29b434-bc6c-45f4-a5be-b4e028f969e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DistilBert\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "params={\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\":1,\n",
    "        \"NUM_TRAIN_EPOCHS\":8,\n",
    "        \"LEARNING_RATE\":2e-5,\n",
    "        \"WARMUP_PROPORTION\":0.1,\n",
    "        \"MAX_GRAD_NORM\":5,\n",
    "        \"MAX_SEQ_LENGTH\":MAX_SEQ_LENGTH,\n",
    "        \"BATCH_SIZE\":16,\n",
    "        \"NUM_WARMUP_STEPS\":600\n",
    "}\n",
    "df_transformers=df_transformers.append({\"MODEL\":\"DISTILBERT\",\n",
    "                                        \"MODEL_NAME\":model_name,\n",
    "                                        \"TOKENIZER\":DistilBertTokenizer,\n",
    "                                        \"CLASSIFIER\":DistilBertForSequenceClassification,\n",
    "                                        \"OUTPUT_DIR\":OUTPUT_DIR,\n",
    "                                        \"MODEL_FILE_NAME\":model_name+\"_\"+MODEL_FILE_NAME,\n",
    "                                        \"PARAMETERS\":params\n",
    "                                        },\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea4c71-6af8-4375-b21c-8946d05d9003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bert-base\n",
    "model_name = \"bert-base-uncased\"\n",
    "params={\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\":1,\n",
    "        \"NUM_TRAIN_EPOCHS\":3,\n",
    "        \"LEARNING_RATE\":6e-4,\n",
    "        \"WARMUP_PROPORTION\":0.1,\n",
    "        \"MAX_GRAD_NORM\":5,\n",
    "        \"MAX_SEQ_LENGTH\":MAX_SEQ_LENGTH,\n",
    "        \"BATCH_SIZE\":16,\n",
    "        \"NUM_WARMUP_STEPS\":24000\n",
    "}\n",
    "df_transformers=df_transformers.append({\"MODEL\":\"BERT\",\n",
    "                                        \"MODEL_NAME\":model_name,\n",
    "                                        \"TOKENIZER\":BertTokenizer,\n",
    "                                        \"CLASSIFIER\":BertForSequenceClassification,\n",
    "                                        \"OUTPUT_DIR\":OUTPUT_DIR,\n",
    "                                        \"MODEL_FILE_NAME\":model_name+\"_\"+MODEL_FILE_NAME,\n",
    "                                        \"PARAMETERS\":params\n",
    "                                        },\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f37b7-4b46-4c7a-aab1-ae58986b5dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bert-large\n",
    "model_name = \"bert-large-uncased\"\n",
    "params={\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\":1,\n",
    "        \"NUM_TRAIN_EPOCHS\":3,\n",
    "        \"LEARNING_RATE\":6e-4,\n",
    "        \"WARMUP_PROPORTION\":0.1,\n",
    "        \"MAX_GRAD_NORM\":5,\n",
    "        \"MAX_SEQ_LENGTH\":MAX_SEQ_LENGTH,\n",
    "        \"BATCH_SIZE\":16,\n",
    "        \"NUM_WARMUP_STEPS\":24000\n",
    "}\n",
    "df_transformers=df_transformers.append({\"MODEL\":\"BERT-LARGE\",\n",
    "                                        \"MODEL_NAME\":model_name,\n",
    "                                        \"TOKENIZER\":BertTokenizer,\n",
    "                                        \"CLASSIFIER\":BertForSequenceClassification,\n",
    "                                        \"OUTPUT_DIR\":OUTPUT_DIR,\n",
    "                                        \"MODEL_FILE_NAME\":model_name+\"_\"+MODEL_FILE_NAME,\n",
    "                                        \"PARAMETERS\":params\n",
    "                                        },\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c06cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robertabert\n",
    "model_name = \"roberta-base\"\n",
    "params={\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\":1,\n",
    "        \"NUM_TRAIN_EPOCHS\":3,\n",
    "        \"LEARNING_RATE\":6e-4,\n",
    "        \"WARMUP_PROPORTION\":0.1,\n",
    "        \"MAX_GRAD_NORM\":5,\n",
    "        \"MAX_SEQ_LENGTH\":MAX_SEQ_LENGTH,\n",
    "        \"BATCH_SIZE\":16,\n",
    "        \"NUM_WARMUP_STEPS\":24000\n",
    "}\n",
    "df_transformers=df_transformers.append({\"MODEL\":\"ROBERTA\",\n",
    "                                        \"MODEL_NAME\":model_name,\n",
    "                                        \"TOKENIZER\":RobertaTokenizer,\n",
    "                                        \"CLASSIFIER\":RobertaForSequenceClassification,\n",
    "                                        \"OUTPUT_DIR\":OUTPUT_DIR,\n",
    "                                        \"MODEL_FILE_NAME\":model_name+\"_\"+MODEL_FILE_NAME,\n",
    "                                        \"PARAMETERS\":params\n",
    "                                        },\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab598409-0980-4eee-b910-aaa7c618664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLMRobertabert\n",
    "model_name = \"xlm-roberta-large\"\n",
    "params={\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\":1,\n",
    "        \"NUM_TRAIN_EPOCHS\":3,\n",
    "        \"LEARNING_RATE\":6e-4,\n",
    "        \"WARMUP_PROPORTION\":0.1,\n",
    "        \"MAX_GRAD_NORM\":5,\n",
    "        \"MAX_SEQ_LENGTH\":MAX_SEQ_LENGTH,\n",
    "        \"BATCH_SIZE\":16,\n",
    "        \"NUM_WARMUP_STEPS\":24000\n",
    "}\n",
    "df_transformers=df_transformers.append({\"MODEL\":\"XLMROBERTA\",\n",
    "                                        \"MODEL_NAME\":model_name,\n",
    "                                        \"TOKENIZER\":XLMRobertaTokenizer,\n",
    "                                        \"CLASSIFIER\":XLMRobertaForSequenceClassification,\n",
    "                                        \"OUTPUT_DIR\":OUTPUT_DIR,\n",
    "                                        \"MODEL_FILE_NAME\":model_name+\"_\"+MODEL_FILE_NAME,\n",
    "                                        \"PARAMETERS\":params\n",
    "                                        },\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbe9d3-771c-4500-b411-e9615de49796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Albert\n",
    "model_name = \"albert-base-v2\"\n",
    "params={\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\":1,\n",
    "        \"NUM_TRAIN_EPOCHS\":3,\n",
    "        \"LEARNING_RATE\":6e-4,\n",
    "        \"WARMUP_PROPORTION\":0.1,\n",
    "        \"MAX_GRAD_NORM\":5,\n",
    "        \"MAX_SEQ_LENGTH\":MAX_SEQ_LENGTH,\n",
    "        \"BATCH_SIZE\":16,\n",
    "        \"NUM_WARMUP_STEPS\":24000\n",
    "}\n",
    "df_transformers=df_transformers.append({\"MODEL\":\"ALBERT\",\n",
    "                                        \"MODEL_NAME\":model_name,\n",
    "                                        \"TOKENIZER\":AlbertTokenizer,\n",
    "                                        \"CLASSIFIER\":AlbertForSequenceClassification,\n",
    "                                        \"OUTPUT_DIR\":OUTPUT_DIR,\n",
    "                                        \"MODEL_FILE_NAME\":model_name+\"_\"+MODEL_FILE_NAME,\n",
    "                                        \"PARAMETERS\":params\n",
    "                                        },\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805ca25-1695-4553-8169-e57b5663323e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de74850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize bert model  \n",
    "def train(ts,train_texts,train_labels,dev_texts,dev_labels,target_names,label2idx):\n",
    "    \n",
    "    tokenizer =ts['TOKENIZER'].from_pretrained(ts['MODEL_NAME'],target_names=target_names)\n",
    "    # Using trained model\n",
    "    model=ts['CLASSIFIER'].from_pretrained(ts['MODEL_NAME'],num_labels = len(target_names),\n",
    "                                                                output_attentions = False,\n",
    "                                                                output_hidden_states = False)     \n",
    "    ## Prepare for data loading and parameter setting for bert model\n",
    "    train_features = convert_examples_to_inputs(train_texts,train_labels, label2idx, ts['PARAMETERS']['MAX_SEQ_LENGTH'], tokenizer)\n",
    "    train_dataloader = get_data_loader(train_features, ts['PARAMETERS']['MAX_SEQ_LENGTH'], ts['PARAMETERS']['BATCH_SIZE'], shuffle=False)\n",
    "    dev_features = convert_examples_to_inputs(dev_texts,dev_labels, label2idx, ts['PARAMETERS']['MAX_SEQ_LENGTH'], tokenizer)\n",
    "    dev_dataloader = get_data_loader(dev_features, ts['PARAMETERS']['MAX_SEQ_LENGTH'], ts['PARAMETERS']['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "    num_train_steps = int(len(train_dataloader.dataset) / ts['PARAMETERS']['BATCH_SIZE'] /ts['PARAMETERS']['GRADIENT_ACCUMULATION_STEPS'] * params['NUM_TRAIN_EPOCHS'])\n",
    "    num_warmup_steps = ts['PARAMETERS']['NUM_WARMUP_STEPS']\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=ts['PARAMETERS']['LEARNING_RATE'], correct_bias=False)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,num_training_steps=num_train_steps)\n",
    "\n",
    "    ##Enable GPU if has\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    ## Start to training \n",
    "    torch.backends.cudnn.benchmark = True # tuning guide:https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "\n",
    "    loss_history = []\n",
    "    no_improvement = 0\n",
    "    PATIENCE=2\n",
    "    for _ in trange(int(ts['PARAMETERS'][\"NUM_TRAIN_EPOCHS\"]), desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        #j=0\n",
    "        #sample_ids=[]\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids,sample_ids = batch\n",
    "\n",
    "            if ts['MODEL']!=\"DISTILBERT\":\n",
    "                outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids) # non-distillbert\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask=input_mask,labels=label_ids)\n",
    "\n",
    "            #if aum:\n",
    "            #    records = aum_calculator.update(outputs[1], label_ids,sample_ids.tolist())\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if ts['PARAMETERS']['GRADIENT_ACCUMULATION_STEPS'] > 1:\n",
    "                loss = loss / ts['PARAMETERS']['GRADIENT_ACCUMULATION_STEPS']\n",
    "\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % ts['PARAMETERS']['GRADIENT_ACCUMULATION_STEPS'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),ts['PARAMETERS']['MAX_GRAD_NORM'])  \n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "        dev_loss, _, _ = evaluate(model, dev_dataloader)\n",
    "         #print(\"Dev loss:\", dev_loss)\n",
    "\n",
    "        print(\"Loss history:\", loss_history)\n",
    "        print(\"Dev loss:\", dev_loss)\n",
    "\n",
    "        if len(loss_history) == 0 or dev_loss < min(loss_history):\n",
    "            no_improvement = 0\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            output_model_file = os.path.join(ts['OUTPUT_DIR'], ts['MODEL_FILE_NAME'])\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= PATIENCE: \n",
    "            print(\"No improvement on development set. Finish training.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        loss_history.append(dev_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the dataset based on trained distilbert model\n",
    "def data_evaluation(ts,texts,labels,target_names,label2idx):\n",
    "    # Convert test data of submission to features\n",
    "    #target_names = list(set(labels))\n",
    "    #label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "\n",
    "    # Enable GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Select bert model\n",
    "    #BERT_MODEL = \"distilbert-base-uncased\"\n",
    "    tokenizer = ts['TOKENIZER'].from_pretrained(ts['MODEL_NAME'])\n",
    "\n",
    "    # Using trained model\n",
    "    model_state_dict = torch.load(os.path.join(ts['OUTPUT_DIR'], ts['MODEL_FILE_NAME']),\n",
    "                                  map_location=lambda storage, loc: storage)\n",
    "    model=ts['CLASSIFIER'].from_pretrained(ts['MODEL_NAME'], state_dict=model_state_dict, num_labels = len(target_names),\n",
    "                                                                    output_attentions = False,\n",
    "                                                                    output_hidden_states = False)    \n",
    "    model.to(device)\n",
    "\n",
    "    # Convert text and labels to embeddings \n",
    "    features = convert_examples_to_inputs(texts, labels, label2idx,  ts['PARAMETERS']['MAX_SEQ_LENGTH'], tokenizer)\n",
    "    dataloader = get_data_loader(features, ts['PARAMETERS']['MAX_SEQ_LENGTH'], ts['PARAMETERS']['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    # Predict the result, and discard the evaluatoin result, only take the prediction result.\n",
    "    _, correct, predicted = evaluate(model, dataloader)\n",
    "    print(\"Errors performance:\", precision_recall_fscore_support(correct, predicted, average=\"micro\"))\n",
    "\n",
    "    bert_accuracy = np.mean(predicted == correct)\n",
    "\n",
    "    #print(round(bert_accuracy,2))\n",
    "    print(classification_report(correct, predicted))\n",
    "    return round(bert_accuracy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0d42e-8b9e-421a-886c-cb9933891d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc=pd.DataFrame(columns=[\"Model\",\"Accuracy\"])\n",
    "for i in range(len(df_transformers)):\n",
    "    train(df_transformers.iloc[i],train_texts,train_labels,dev_texts,dev_labels,target_names,label2idx)\n",
    "    accuracy=data_evaluation(df_transformers.iloc[i],test_texts,test_labels,target_names,label2idx)\n",
    "    df_acc=df_acc.append({\"Model\":df_transformers.iloc[i][\"MODEL\"],\n",
    "                          \"Accuracy\":accuracy},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517578b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Loading the test data for submission\n",
    "sub_data_path=\"./01_data/WikiLarge_Test.csv\"\n",
    "sub_data=pd.read_csv(sub_data_path)\n",
    "\n",
    "sub_texts=list(sub_data[\"original_text\"])\n",
    "sub_labels=[random.choice([0,1]) for i in range(len(sub_texts))]\n",
    "\n",
    "print(\"Submission Test size:\", len(sub_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcfadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,sub_predicted,_=data_evaluation(sub_texts,sub_labels,model_name,params,trained=True,OUTPUT_DIR = OUTPUT_DIR, MODEL_FILE_NAME = MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0945ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Produce the submission file\n",
    "df_sub=pd.DataFrame(columns=[\"id\",\"label\"])\n",
    "df_sub['label']=sub_predicted\n",
    "df_sub['id']=[i for i in range(len(sub_predicted))]\n",
    "df_sub.to_csv(\"./tmp/submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "582bac1d02c6c8cbd27dd91c74044d14e2b63f14cda36d56af554a884b3aa477"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
